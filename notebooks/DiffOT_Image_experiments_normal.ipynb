{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Code for experiments with colored MNIST and Celeba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T16:23:46.719000Z",
     "iopub.status.busy": "2024-05-23T16:23:46.718088Z",
     "iopub.status.idle": "2024-05-23T16:23:48.894793Z",
     "shell.execute_reply": "2024-05-23T16:23:48.894147Z",
     "shell.execute_reply.started": "2024-05-23T16:23:46.718932Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# This needed to use dataloaders for some datasets\n",
    "from PIL import PngImagePlugin\n",
    "from tqdm import trange\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "LARGE_ENOUGH_NUMBER = 100\n",
    "PngImagePlugin.MAX_TEXT_CHUNK = LARGE_ENOUGH_NUMBER * (1024**2)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T16:23:48.896123Z",
     "iopub.status.busy": "2024-05-23T16:23:48.895817Z",
     "iopub.status.idle": "2024-05-23T16:23:48.971446Z",
     "shell.execute_reply": "2024-05-23T16:23:48.970737Z",
     "shell.execute_reply.started": "2024-05-23T16:23:48.896107Z"
    }
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Config\n",
    "\n",
    "Dataset choosing in the first rows\n",
    "\n",
    "Continue training in the last rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T16:23:55.677258Z",
     "iopub.status.busy": "2024-05-23T16:23:55.676519Z",
     "iopub.status.idle": "2024-05-23T16:23:55.692231Z",
     "shell.execute_reply": "2024-05-23T16:23:55.691191Z",
     "shell.execute_reply.started": "2024-05-23T16:23:55.677211Z"
    }
   },
   "outputs": [],
   "source": [
    "# For Celeba exps\n",
    "# DATASET1, DATASET1_PATH = \"CelebA_low\", \"/home/zyz/data/img_align_celeba\"\n",
    "# DATASET2, DATASET2_PATH = \"CelebA_high\", \"/home/zyz/data/img_align_celeba\"\n",
    "\n",
    "# For Colored MNIST exps\n",
    "DATASET1, DATASET1_PATH = \"MNIST-colored_2\", \"~/data/MNIST\"\n",
    "DATASET2, DATASET2_PATH = \"MNIST-colored_3\", \"~/data/MNIST\"\n",
    "\n",
    "DATASET1_CHANNELS = 3\n",
    "DATASET2_CHANNELS = 3\n",
    "\n",
    "SEED = 0xBADBEEF\n",
    "\n",
    "# GPU choosing\n",
    "DEVICE_IDS = [0]\n",
    "\n",
    "# whether continue training, -1 means re-training, N > -1 means continue at sdeN or iteration N\n",
    "# In this training strategy, the \"sde\" doesn't matter\n",
    "CONTINUE = {\"sde\": -1, \"iteration\": -1}\n",
    "\n",
    "\n",
    "# We use epsilon in [0, 1, 10]\n",
    "EPSILON = 0\n",
    "\n",
    "# N steps in the Euler-Maruyama(step number of SDE shift and noise)\n",
    "N_STEPS = 10\n",
    "# COST = \"schrodinger\"\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "# the step number adding noise in diffusion process style\n",
    "DIFFUSION_STEPS = 1000\n",
    "PIVOTAL_LIST = [0, 50, 100]\n",
    "\n",
    "MAX_STEPS = 2001  # MAX_STEPS = 100001\n",
    "INNER_ITERS = 10\n",
    "\n",
    "LOG_INTERVAL = 50\n",
    "PLOT_INTERVAL = 50\n",
    "CPKT_INTERVAL = 100\n",
    "\n",
    "\n",
    "# All hyperparameters below is set to the values used for the experiments, which discribed in the article\n",
    "BETA_NET_LR, SDE_LR = 1e-4, 1e-4\n",
    "BETA_NET_BETA, SDE_BETA = 0.9, 0.9\n",
    "SDE_GRADIENT_MAX_NORM = float(\"inf\")\n",
    "BETA_NET_GRADIENT_MAX_NORM = float(\"inf\")\n",
    "\n",
    "\n",
    "IMG_SIZE = 32\n",
    "UNET_BASE_FACTOR = 128\n",
    "\n",
    "TIME_DIM = 128\n",
    "USE_POSITIONAL_ENCODING = True\n",
    "INTEGRAL_SCALE = 1 / (3 * IMG_SIZE * IMG_SIZE)\n",
    "\n",
    "PREDICT_SHIFT = True\n",
    "USE_GRADIENT_CHECKPOINT = False\n",
    "N_LAST_STEPS_WITHOUT_NOISE = 1\n",
    "EPSILON_SCHEDULER_LAST_ITER = 20000\n",
    "IMAGE_INPUT = True\n",
    "\n",
    "AUGMENTED_DATASETS = [\"dtd\"]\n",
    "FID_EPOCHS = 50 if DATASET1 in AUGMENTED_DATASETS else 1\n",
    "assert torch.cuda.is_available()\n",
    "torch.cuda.set_device(f\"cuda:{DEVICE_IDS[0]}\")\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T16:23:56.201116Z",
     "iopub.status.busy": "2024-05-23T16:23:56.200588Z",
     "iopub.status.idle": "2024-05-23T16:23:56.210002Z",
     "shell.execute_reply": "2024-05-23T16:23:56.208938Z",
     "shell.execute_reply.started": "2024-05-23T16:23:56.201081Z"
    }
   },
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "EXP_NAME = f\"Normal_{DATASET1}_to_{DATASET2}_pivotal_{'_'.join(map(str, PIVOTAL_LIST))}_{current_time}\"\n",
    "\n",
    "OUTPUT_PATH = f\"../logs/{EXP_NAME}/\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    filename=os.path.join(OUTPUT_PATH, \"training.log\"),\n",
    "    format=\"%(asctime)s - [line:%(lineno)d] - %(levelname)s: %(message)s\",\n",
    "    filemode=\"w\",  # 这里的'w'代表写模式，如果用'a'，则为追加模式\n",
    ")\n",
    "\n",
    "logging.info(f\"{EXP_NAME = }\")\n",
    "logging.info(f\"{OUTPUT_PATH = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T16:23:56.509372Z",
     "iopub.status.busy": "2024-05-23T16:23:56.508580Z",
     "iopub.status.idle": "2024-05-23T16:23:56.634652Z",
     "shell.execute_reply": "2024-05-23T16:23:56.633666Z",
     "shell.execute_reply.started": "2024-05-23T16:23:56.509315Z"
    }
   },
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    DATASET1=DATASET1,\n",
    "    DATASET1_PATH=DATASET1_PATH,\n",
    "    DATASET2=DATASET2,\n",
    "    DATASET2_PATH=DATASET2_PATH,\n",
    "    DATASET1_CHANNELS=DATASET1_CHANNELS,\n",
    "    DATASET2_CHANNELS=DATASET2_CHANNELS,\n",
    "    SEED=SEED,\n",
    "    EPSILON=EPSILON,\n",
    "    N_STEPS=N_STEPS,\n",
    "    BATCH_SIZE=BATCH_SIZE,\n",
    "    DIFFUSION_STEPS=DIFFUSION_STEPS,\n",
    "    PIVOTAL_LIST=PIVOTAL_LIST,\n",
    "    MAX_STEPS=MAX_STEPS,\n",
    "    INNER_ITERS=INNER_ITERS,\n",
    "    LOG_INTERVAL=LOG_INTERVAL,\n",
    "    PLOT_INTERVAL=PLOT_INTERVAL,\n",
    "    CPKT_INTERVAL=CPKT_INTERVAL,\n",
    "    BETA_NET_LR=BETA_NET_LR,\n",
    "    SDE_LR=SDE_LR,\n",
    "    BETA_NET_BETA=BETA_NET_BETA,\n",
    "    SDE_BETA=SDE_BETA,\n",
    "    SDE_GRADIENT_MAX_NORM=SDE_GRADIENT_MAX_NORM,\n",
    "    BETA_NET_GRADIENT_MAX_NORM=BETA_NET_GRADIENT_MAX_NORM,\n",
    "    IMG_SIZE=IMG_SIZE,\n",
    "    UNET_BASE_FACTOR=UNET_BASE_FACTOR,\n",
    "    TIME_DIM=TIME_DIM,\n",
    "    USE_POSITIONAL_ENCODING=USE_POSITIONAL_ENCODING,\n",
    "    INTEGRAL_SCALE=INTEGRAL_SCALE,\n",
    "    PREDICT_SHIFT=PREDICT_SHIFT,\n",
    "    USE_GRADIENT_CHECKPOINT=USE_GRADIENT_CHECKPOINT,\n",
    "    N_LAST_STEPS_WITHOUT_NOISE=N_LAST_STEPS_WITHOUT_NOISE,\n",
    "    EPSILON_SCHEDULER_LAST_ITER=EPSILON_SCHEDULER_LAST_ITER,\n",
    "    IMAGE_INPUT=IMAGE_INPUT,\n",
    "    FID_EPOCHS=FID_EPOCHS,\n",
    ")\n",
    "\n",
    "with open(os.path.join(OUTPUT_PATH, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f, ensure_ascii=False)\n",
    "logging.info(f\"Config: \\n{json.dumps(config, indent=4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Function definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### network generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T16:23:57.000282Z",
     "iopub.status.busy": "2024-05-23T16:23:56.999833Z",
     "iopub.status.idle": "2024-05-23T16:23:57.006218Z",
     "shell.execute_reply": "2024-05-23T16:23:57.005519Z",
     "shell.execute_reply.started": "2024-05-23T16:23:57.000253Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.cunet import CUNet\n",
    "from src.enot import SDE, integrate\n",
    "from src.resnet2 import ResNet_D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data and pivotal sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-05-23T16:23:57.368911Z",
     "iopub.status.busy": "2024-05-23T16:23:57.368320Z",
     "iopub.status.idle": "2024-05-23T16:23:58.751916Z",
     "shell.execute_reply": "2024-05-23T16:23:58.751259Z",
     "shell.execute_reply.started": "2024-05-23T16:23:57.368871Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from diffusers import DDIMScheduler\n",
    "\n",
    "from src import distributions\n",
    "\n",
    "\n",
    "def sample_all_pivotal(\n",
    "    source_sampler: distributions.Sampler,\n",
    "    target_sampler: distributions.Sampler,\n",
    "    batch_size: int = 4,\n",
    "    diffusion_steps: int = 1000,\n",
    "    pivotal_list: list[int] = [0, 20, 50, 100],\n",
    ") -> list[torch.Tensor]:\n",
    "    scheduler = DDIMScheduler(num_train_timesteps=diffusion_steps)\n",
    "    pivotal_path = []\n",
    "\n",
    "    source: torch.Tensor = source_sampler.sample(batch_size)\n",
    "    target: torch.Tensor = target_sampler.sample(batch_size)\n",
    "    source_list = [source]\n",
    "    target_list = [target]\n",
    "    for i in range(min(diffusion_steps, pivotal_list[-1])):\n",
    "        source = scheduler.add_noise(\n",
    "            source, torch.randn_like(source), torch.Tensor([i]).long()\n",
    "        )\n",
    "        target = scheduler.add_noise(\n",
    "            target, torch.randn_like(target), torch.Tensor([i]).long()\n",
    "        )\n",
    "        if (i + 1) in pivotal_list:\n",
    "            source_list.append(source)\n",
    "            target_list.append(target)\n",
    "\n",
    "    target_list.reverse()\n",
    "\n",
    "    pivotal_path.extend(source_list)\n",
    "    pivotal_path.extend(target_list[1:])\n",
    "\n",
    "    return pivotal_path\n",
    "\n",
    "\n",
    "def sample_step_t_pivotal(\n",
    "    source_sampler: distributions.Sampler,\n",
    "    target_sampler: distributions.Sampler,\n",
    "    batch_size: int = 4,\n",
    "    diffusion_steps: int = 1000,\n",
    "    pivotal_list: list[int] = [0, 200, 500, 1000],\n",
    "    pivotal_step: int = 0,\n",
    "):\n",
    "    pivotal_path = sample_all_pivotal(\n",
    "        source_sampler, target_sampler, batch_size, diffusion_steps, pivotal_list\n",
    "    )\n",
    "    pivotal_t, pivotal_tadd1 = (\n",
    "        pivotal_path[pivotal_step],\n",
    "        pivotal_path[pivotal_step + 1],\n",
    "    )\n",
    "    return pivotal_t, pivotal_tadd1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapping plotters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-05-23T16:23:58.753369Z",
     "iopub.status.busy": "2024-05-23T16:23:58.753072Z",
     "iopub.status.idle": "2024-05-23T16:23:58.768445Z",
     "shell.execute_reply": "2024-05-23T16:23:58.767605Z",
     "shell.execute_reply.started": "2024-05-23T16:23:58.753352Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 采样关键点\n",
    "def draw_all_pivotal(\n",
    "    source: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    diffusion_steps: int = 1000,\n",
    "    pivotal_list: list[int] = [0, 20, 50, 100],\n",
    ") -> list:\n",
    "    scheduler = DDIMScheduler(num_train_timesteps=diffusion_steps)\n",
    "    pivotal_path = []\n",
    "\n",
    "    source_list = [source]\n",
    "    target_list = [target]\n",
    "    for i in range(min(diffusion_steps, pivotal_list[-1])):\n",
    "        source = scheduler.add_noise(\n",
    "            source, torch.randn_like(source), torch.Tensor([i]).long()\n",
    "        )\n",
    "        target = scheduler.add_noise(\n",
    "            target, torch.randn_like(target), torch.Tensor([i]).long()\n",
    "        )\n",
    "        if (i + 1) in pivotal_list:\n",
    "            source_list.append(source)\n",
    "            target_list.append(target)\n",
    "\n",
    "    target_list.reverse()\n",
    "\n",
    "    pivotal_path.extend(source_list)\n",
    "    pivotal_path.extend(target_list[1:])\n",
    "\n",
    "    imgs: np.ndarray = (\n",
    "        torch.stack(pivotal_path)\n",
    "        .to(\"cpu\")\n",
    "        .permute(0, 2, 3, 1)\n",
    "        .mul(0.5)\n",
    "        .add(0.5)\n",
    "        .numpy()\n",
    "        .clip(0, 1)\n",
    "    )\n",
    "    nrows, ncols = 1, len(pivotal_path)\n",
    "    fig = plt.figure(figsize=(11 * ncols, 10 * nrows))\n",
    "    for i, img in enumerate(imgs):\n",
    "        ax = fig.add_subplot(nrows, ncols, i + 1)\n",
    "        ax.imshow(img)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_title(f\"X({i})\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def draw_sub_mapping(\n",
    "    source_sampler: distributions.Sampler,\n",
    "    target_sampler: distributions.Sampler,\n",
    "    sde: SDE,\n",
    "    plot_n_samples: int = 4,\n",
    "    diffusion_steps: int = 1000,\n",
    "    pivotal_list: list[int] = [0, 200, 500, 1000],\n",
    "    pivotal_step: int = 0,\n",
    "    saving_path: str | None = None,\n",
    "):\n",
    "    clear_output()\n",
    "    source, target = sample_step_t_pivotal(\n",
    "        source_sampler,\n",
    "        target_sampler,\n",
    "        plot_n_samples,\n",
    "        diffusion_steps,\n",
    "        pivotal_list,\n",
    "        pivotal_step,\n",
    "    )\n",
    "\n",
    "    tr, _, _ = sde(source)\n",
    "    # print(f\"[Debug] {tr.shape = }\")\n",
    "    mapped = tr[:, -1, :]\n",
    "    # print(f\"[Debug] {mapped.shape = }\") # shape (plot_n_sample, 3, 32, 32)\n",
    "\n",
    "    n_imgs: np.ndarray = (\n",
    "        torch.stack([source, target, mapped])\n",
    "        .to(\"cpu\")\n",
    "        .permute(1, 0, 3, 4, 2)\n",
    "        .mul(0.5)\n",
    "        .add(0.5)\n",
    "        .numpy()\n",
    "        .clip(0, 1)\n",
    "    )  # shpae = (plot_n_samples, 3, [h, w, c]) 3->(source, target, mapped)\n",
    "    nrows, ncols = plot_n_samples, 3\n",
    "    fig = plt.figure(figsize=(10 * ncols, 10 * nrows))\n",
    "    for i, imgs in enumerate(n_imgs):\n",
    "        for j, img in enumerate(imgs):\n",
    "            ax = fig.add_subplot(plot_n_samples, 3, i * ncols + j + 1)\n",
    "            ax.imshow(img)\n",
    "            ax.set_axis_off()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if saving_path:\n",
    "        plt.savefig(os.path(saving_path))\n",
    "    plt.show()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def draw_linked_mapping(\n",
    "    source_sampler: distributions.Sampler,\n",
    "    target_sampler: distributions.Sampler,\n",
    "    SDEs: list[SDE],\n",
    "    plot_n_samples: int = 4,\n",
    "):\n",
    "    device = next(SDEs[0].parameters()).device\n",
    "    source_dataset, target_dataset, mapped_dataset = [], [], []\n",
    "    pivotals_list = []\n",
    "    for i in range(plot_n_samples):\n",
    "        source, target = (\n",
    "            source_sampler.sample(1).to(device),\n",
    "            target_sampler.sample(1).to(device),\n",
    "        )\n",
    "\n",
    "        pivotals = [source.clone().detach()]\n",
    "        for t in range(len(SDEs)):\n",
    "            x0 = pivotals[t]\n",
    "            trajectory, times, _ = SDEs[t](x0)\n",
    "            xN = trajectory[:, -1, :]\n",
    "            pivotals.append(xN)\n",
    "\n",
    "        mapped_dataset.append(pivotals[-1])\n",
    "        source_dataset.append(source)\n",
    "        target_dataset.append(target)\n",
    "\n",
    "        pivotals.append(target.clone().detach())\n",
    "        pivotals_list.append(pivotals)\n",
    "\n",
    "        fig, axes = plt.subplots(1, len(SDEs) + 2, figsize=(5 * (len(SDEs) + 2), 5))\n",
    "        axes = axes.flatten()\n",
    "        for pivotal, ax in zip(pivotals, axes):\n",
    "            img = torch.squeeze(pivotal).to(\"cpu\")\n",
    "            img = img.permute(1, 2, 0).mul(0.5).add(0.5).numpy().clip(0, 1)\n",
    "            ax.imshow(img)\n",
    "\n",
    "        fig.tight_layout(pad=0.001)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return pivotals_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T16:23:58.769318Z",
     "iopub.status.busy": "2024-05-23T16:23:58.769129Z",
     "iopub.status.idle": "2024-05-23T16:23:59.178108Z",
     "shell.execute_reply": "2024-05-23T16:23:59.177224Z",
     "shell.execute_reply.started": "2024-05-23T16:23:58.769299Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.tools import freeze, unfreeze, weights_init_D\n",
    "\n",
    "\n",
    "def epsilon_scheduler(step):\n",
    "    return min(EPSILON, EPSILON * (step / EPSILON_SCHEDULER_LAST_ITER))\n",
    "\n",
    "\n",
    "# linked mapping training\n",
    "def training_linked_mapping(\n",
    "    source_sampler: distributions.Sampler,\n",
    "    target_sampler: distributions.Sampler,\n",
    "    SDEs: list[SDE | torch.nn.DataParallel | torch.nn.Module],\n",
    "    BETA_NETs: list[ResNet_D | torch.nn.Module | torch.nn.Module],\n",
    "    SDE_OPTs: list[torch.optim.Optimizer],\n",
    "    BETA_NET_OPTs: list[torch.optim.Optimizer],\n",
    "    SDE_SCHEDULERs: list[torch.optim.lr_scheduler.LRScheduler],\n",
    "    BETA_NET_SCHEDULERs: list[torch.optim.lr_scheduler.LRScheduler],\n",
    "    iterations: int = 1000,\n",
    "    inner_iterations: int = 10,\n",
    "    diffusion_steps: int = 1000,\n",
    "    pivotal_list: list[int] = [0, 200, 500, 1000],\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    continue_iteration: int = CONTINUE[\"iteration\"],\n",
    "):\n",
    "    T = len(SDEs)\n",
    "    for i in trange(max(0, continue_iteration), iterations):\n",
    "        # interval events\n",
    "        if i % LOG_INTERVAL == 0:\n",
    "            logging.info(f\"-------------- Iteration={i+1}/{iterations} --------------\")\n",
    "            # TODO: log FID score\n",
    "\n",
    "        if i != 0 and i % CPKT_INTERVAL == 0:\n",
    "            CKPT_DIR = os.path.join(OUTPUT_PATH, f\"iter{i}/\")\n",
    "            os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "            for tt in range(T):\n",
    "                torch.save(\n",
    "                    SDEs[tt].state_dict(),\n",
    "                    os.path.join(CKPT_DIR, f\"sde{tt}.pt\"),\n",
    "                )\n",
    "                torch.save(\n",
    "                    SDE_OPTs[tt].state_dict(), os.path.join(CKPT_DIR, f\"sde_opt{tt}.pt\")\n",
    "                )\n",
    "                torch.save(\n",
    "                    SDE_SCHEDULERs[tt].state_dict(),\n",
    "                    os.path.join(CKPT_DIR, f\"sde_scheduler{tt}.pt\"),\n",
    "                )\n",
    "\n",
    "                torch.save(\n",
    "                    BETA_NETs[tt].state_dict(),\n",
    "                    os.path.join(CKPT_DIR, f\"beta_net{tt}.pt\"),\n",
    "                )\n",
    "                torch.save(\n",
    "                    BETA_NET_OPTs[tt].state_dict(),\n",
    "                    os.path.join(CKPT_DIR, f\"beta_net_opt{tt}.pt\"),\n",
    "                )\n",
    "                torch.save(\n",
    "                    BETA_NET_SCHEDULERs[tt].state_dict(),\n",
    "                    os.path.join(CKPT_DIR, f\"beta_net_scheduler{tt}.pt\"),\n",
    "                )\n",
    "                logging.info(f\"Checkpoint saving at iteration {i}\")\n",
    "\n",
    "        if i % PLOT_INTERVAL == 0:\n",
    "            clear_output()\n",
    "            draw_linked_mapping(\n",
    "                source_sampler,\n",
    "                target_sampler,\n",
    "                SDEs,\n",
    "                1,\n",
    "            )\n",
    "\n",
    "        # training sde\n",
    "        for t in range(T):\n",
    "            logging.info(f\"training sde{t}\")\n",
    "            sde, beta_net, sde_opt, beta_net_opt, sde_scheduler, beta_net_scheduler = (\n",
    "                SDEs[t],\n",
    "                BETA_NETs[t],\n",
    "                SDE_OPTs[t],\n",
    "                BETA_NET_OPTs[t],\n",
    "                SDE_SCHEDULERs[t],\n",
    "                BETA_NET_SCHEDULERs[t],\n",
    "            )\n",
    "\n",
    "            # Sub mapping training\n",
    "            # Optim beta network\n",
    "            freeze(sde)\n",
    "            unfreeze(beta_net)\n",
    "            beta_net.train()\n",
    "\n",
    "            # clear grad\n",
    "            beta_net_opt.zero_grad()\n",
    "\n",
    "            x0, xT = sample_step_t_pivotal(\n",
    "                source_sampler,\n",
    "                target_sampler,\n",
    "                batch_size,\n",
    "                diffusion_steps,\n",
    "                pivotal_list,\n",
    "                t,\n",
    "            )\n",
    "            x0.requires_grad_()\n",
    "\n",
    "            # forward\n",
    "            trajectory, times, shifts = sde(x0)\n",
    "            xT_mapped = trajectory[:, -1, :]\n",
    "            # loss\n",
    "            norm = torch.norm(shifts.flatten(start_dim=2), p=2, dim=-1) ** 2\n",
    "            integral = INTEGRAL_SCALE * integrate(norm, times[0])\n",
    "            loss_beta = (-integral - beta_net(xT) + beta_net(xT_mapped)).mean()\n",
    "            # backward\n",
    "            loss_beta.backward()\n",
    "            # clip gradient\n",
    "            beta_net_grad_norm = torch.nn.utils.clip_grad_norm_(  # noqa: F841\n",
    "                beta_net.parameters(), max_norm=BETA_NET_GRADIENT_MAX_NORM\n",
    "            )\n",
    "            # update weights\n",
    "            beta_net_opt.step()\n",
    "            # update beta network lr scheduler\n",
    "            beta_net_scheduler.step()\n",
    "\n",
    "            # Optim sde network\n",
    "            freeze(beta_net)\n",
    "            unfreeze(sde)\n",
    "            sde.train()\n",
    "\n",
    "            new_epsilon = epsilon_scheduler(i)\n",
    "            if len(DEVICE_IDS) > 1:\n",
    "                sde.module.set_epsilon(new_epsilon)\n",
    "            else:\n",
    "                sde.set_epsilon(new_epsilon)\n",
    "\n",
    "            # now it's same sample\n",
    "            for ii in range(inner_iterations):\n",
    "                # clear grad\n",
    "                sde_opt.zero_grad()\n",
    "\n",
    "                x0, xT = sample_step_t_pivotal(\n",
    "                    source_sampler,\n",
    "                    target_sampler,\n",
    "                    batch_size,\n",
    "                    diffusion_steps,\n",
    "                    pivotal_list,\n",
    "                    t,\n",
    "                )\n",
    "                x0.requires_grad_()\n",
    "\n",
    "                # forward\n",
    "                trajectory, times, shifts = sde(x0)\n",
    "                xT_mapped = trajectory[:, -1, :]\n",
    "                # loss\n",
    "                norm = torch.norm(shifts.flatten(start_dim=2), p=2, dim=-1) ** 2\n",
    "                integral = INTEGRAL_SCALE * integrate(norm, times[0])\n",
    "                loss_sde = (integral + beta_net(xT) - beta_net(xT_mapped)).mean()\n",
    "                # backward and update weights\n",
    "                loss_sde.backward()\n",
    "                # clip gradient\n",
    "                sde_gradient_norm = torch.nn.utils.clip_grad_norm_(  # noqa: F841\n",
    "                    sde.parameters(), max_norm=SDE_GRADIENT_MAX_NORM\n",
    "                )\n",
    "                # update weights\n",
    "                sde_opt.step()\n",
    "            # update sde network lr scheduler\n",
    "            sde_scheduler.step()\n",
    "        # clear memory cache\n",
    "        del loss_beta, loss_sde, x0, xT, xT_mapped\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initalize data sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T16:23:59.179772Z",
     "iopub.status.busy": "2024-05-23T16:23:59.179451Z",
     "iopub.status.idle": "2024-05-23T16:24:07.043165Z",
     "shell.execute_reply": "2024-05-23T16:24:07.042398Z",
     "shell.execute_reply.started": "2024-05-23T16:23:59.179752Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.tools import load_dataset\n",
    "\n",
    "X_sampler, X_test_sampler = load_dataset(\n",
    "    DATASET1, DATASET1_PATH, img_size=IMG_SIZE, batch_size=BATCH_SIZE, num_workers=8\n",
    ")\n",
    "Y_sampler, Y_test_sampler = load_dataset(\n",
    "    DATASET2, DATASET2_PATH, img_size=IMG_SIZE, batch_size=BATCH_SIZE, num_workers=8\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "clear_output()\n",
    "\n",
    "logging.info(\"Initialize data sampler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### separate fixed X, Y, show sampler data and pivotal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-05-23T16:24:07.044845Z",
     "iopub.status.busy": "2024-05-23T16:24:07.044437Z",
     "iopub.status.idle": "2024-05-23T16:24:07.817878Z",
     "shell.execute_reply": "2024-05-23T16:24:07.817210Z",
     "shell.execute_reply.started": "2024-05-23T16:24:07.044817Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_fixed, Y_fixed = X_sampler.sample(10), Y_sampler.sample(10)\n",
    "\n",
    "X_test_fixed, Y_test_fixed = X_test_sampler.sample(10), Y_test_sampler.sample(10)\n",
    "\n",
    "draw_all_pivotal(X_fixed[0], Y_fixed[0], DIFFUSION_STEPS, PIVOTAL_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T16:24:07.818848Z",
     "iopub.status.busy": "2024-05-23T16:24:07.818660Z",
     "iopub.status.idle": "2024-05-23T16:24:09.782509Z",
     "shell.execute_reply": "2024-05-23T16:24:09.781469Z",
     "shell.execute_reply.started": "2024-05-23T16:24:07.818829Z"
    }
   },
   "outputs": [],
   "source": [
    "SDEs, BETA_NETs = [], []\n",
    "SDE_OPTs, BETA_NET_OPTs = [], []\n",
    "SDE_SCHEDULERs, BETA_NET_SCHEDULERs = [], []\n",
    "\n",
    "for i in range(len(PIVOTAL_LIST) * 2 - 2):\n",
    "    sde = CUNet(\n",
    "        DATASET1_CHANNELS, DATASET2_CHANNELS, TIME_DIM, base_factor=UNET_BASE_FACTOR\n",
    "    ).cuda()\n",
    "\n",
    "    sde = SDE(\n",
    "        shift_model=sde,\n",
    "        epsilon=EPSILON,\n",
    "        n_steps=N_STEPS,\n",
    "        time_dim=TIME_DIM,\n",
    "        n_last_steps_without_noise=N_LAST_STEPS_WITHOUT_NOISE,\n",
    "        use_positional_encoding=USE_POSITIONAL_ENCODING,\n",
    "        use_gradient_checkpoint=USE_GRADIENT_CHECKPOINT,\n",
    "        predict_shift=PREDICT_SHIFT,\n",
    "        image_input=IMAGE_INPUT,\n",
    "    ).cuda()\n",
    "    SDEs.append(sde)\n",
    "\n",
    "    beta_net = ResNet_D(IMG_SIZE, nc=DATASET2_CHANNELS).cuda()\n",
    "    beta_net.apply(weights_init_D)\n",
    "    BETA_NETs.append(beta_net)\n",
    "\n",
    "    sde_opt = torch.optim.Adam(\n",
    "        sde.parameters(), lr=SDE_LR, weight_decay=1e-10, betas=(SDE_BETA, 0.999)\n",
    "    )\n",
    "    beta_net_opt = torch.optim.Adam(\n",
    "        beta_net.parameters(),\n",
    "        lr=BETA_NET_LR,\n",
    "        weight_decay=1e-10,\n",
    "        betas=(BETA_NET_BETA, 0.999),\n",
    "    )\n",
    "    SDE_OPTs.append(sde_opt)\n",
    "    BETA_NET_OPTs.append(beta_net_opt)\n",
    "\n",
    "    sde_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        sde_opt, milestones=[15000, 25000, 40000, 55000, 70000], gamma=0.5\n",
    "    )\n",
    "    beta_net_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        beta_net_opt, milestones=[15000, 25000, 40000, 55000, 70000], gamma=0.5\n",
    "    )\n",
    "    SDE_SCHEDULERs.append(sde_scheduler)\n",
    "    BETA_NET_SCHEDULERs.append(beta_net_scheduler)\n",
    "\n",
    "\n",
    "if len(DEVICE_IDS) > 1 and CONTINUE[\"sde\"] == -1 and CONTINUE[\"iteration\"] == -1:\n",
    "    for i in range(len(SDEs)):\n",
    "        SDEs[i] = nn.DataParallel(SDEs[i], device_ids=DEVICE_IDS)\n",
    "        BETA_NETs[i] = nn.DataParallel(BETA_NETs[i], device_ids=DEVICE_IDS)\n",
    "\n",
    "        print(\"T params:\", np.sum([np.prod(p.shape) for p in SDEs[0].parameters()]))\n",
    "        print(\n",
    "            \"D params:\", np.sum([np.prod(p.shape) for p in BETA_NETs[0].parameters()])\n",
    "        )\n",
    "\n",
    "logging.info(\"Initialize models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load weights for continue training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T16:24:09.784214Z",
     "iopub.status.busy": "2024-05-23T16:24:09.784027Z",
     "iopub.status.idle": "2024-05-23T16:24:09.792112Z",
     "shell.execute_reply": "2024-05-23T16:24:09.791242Z",
     "shell.execute_reply.started": "2024-05-23T16:24:09.784199Z"
    }
   },
   "outputs": [],
   "source": [
    "CKPT_DIR = os.path.join(OUTPUT_PATH, f\"iter{CONTINUE[\"iteration\"]}/\")\n",
    "if CONTINUE[\"iteration\"] > -1:\n",
    "    for i in range(len(SDEs)):\n",
    "        SDEs[i].load_state_dict(torch.load(os.path.join(CKPT_DIR, f\"sde{i}.pt\")))\n",
    "        SDE_OPTs[i].load_state_dict(\n",
    "            torch.load(os.path.join(CKPT_DIR, f\"sde_opt{i}.pt\"))\n",
    "        )\n",
    "        SDE_SCHEDULERs[i].load_state_dict(\n",
    "            torch.load(os.path.join(CKPT_DIR, f\"sde_scheduler{i}.pt\"))\n",
    "        )\n",
    "\n",
    "        BETA_NETs[i].load_state_dict(\n",
    "            torch.load(os.path.join(CKPT_DIR, f\"beta_net{i}.pt\"))\n",
    "        )\n",
    "        BETA_NET_OPTs[i].load_state_dict(\n",
    "            torch.load(os.path.join(CKPT_DIR, f\"beta_net_opt{i}.pt\"))\n",
    "        )\n",
    "        BETA_NET_SCHEDULERs[i].load_state_dict(\n",
    "            torch.load(os.path.join(CKPT_DIR, f\"beta_net_scheduler{i}.pt\"))\n",
    "        )\n",
    "\n",
    "        if len(DEVICE_IDS) > 1:\n",
    "            SDEs[i] = nn.DataParallel(sde, device_ids=DEVICE_IDS)\n",
    "            BETA_NETs[i] = nn.DataParallel(beta_net, device_ids=DEVICE_IDS)\n",
    "\n",
    "    logging.info(f\"Continue training at iteration {CONTINUE[\"iteration\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T16:24:09.793075Z",
     "iopub.status.busy": "2024-05-23T16:24:09.792879Z"
    }
   },
   "outputs": [],
   "source": [
    "training_linked_mapping(\n",
    "    X_sampler,\n",
    "    Y_sampler,\n",
    "    SDEs,\n",
    "    BETA_NETs,\n",
    "    SDE_OPTs,\n",
    "    BETA_NET_OPTs,\n",
    "    SDE_SCHEDULERs,\n",
    "    BETA_NET_SCHEDULERs,\n",
    "    MAX_STEPS,\n",
    "    INNER_ITERS,\n",
    "    DIFFUSION_STEPS,\n",
    "    PIVOTAL_LIST,\n",
    "    BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving model\n",
    "\n",
    "each sub mapping model has been saved at training time, this is option code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-05-23T15:15:20.938725Z",
     "iopub.status.busy": "2024-05-23T15:15:20.938411Z",
     "iopub.status.idle": "2024-05-23T15:15:23.151059Z",
     "shell.execute_reply": "2024-05-23T15:15:23.149962Z",
     "shell.execute_reply.started": "2024-05-23T15:15:20.938706Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, sde in enumerate(SDEs):\n",
    "    path = os.path.join(OUTPUT_PATH, f\"sde{i}.pt\")\n",
    "    torch.save(sde.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metric\n",
    "\n",
    "total test in ../test/test_diffot_image.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_DIR = os.path.join(OUTPUT_PATH, \"iter2000/\")\n",
    "for i, sde in enumerate(SDEs):\n",
    "    path = os.path.join(CKPT_DIR, f\"sde{i}.pt\")\n",
    "    print(f\"{path = }\")\n",
    "    sde.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trans example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pivotals_list shape (plot_n_samples, n_steps, (RGB image))\n",
    "pivotals_list = draw_linked_mapping(\n",
    "    X_sampler,\n",
    "    Y_sampler,\n",
    "    SDEs,\n",
    "    plot_n_samples=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pivotals in pivotals_list:\n",
    "    fig, axes = plt.subplots(1, 8, figsize=(40, 5))\n",
    "    axes = axes.flatten()\n",
    "    for pivotal, ax in zip(pivotals, axes):\n",
    "        img = torch.squeeze(pivotal).to(\"cpu\")\n",
    "        img = img.permute(1, 2, 0).mul(0.5).add(0.5).numpy().clip(0, 1)\n",
    "        ax.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
