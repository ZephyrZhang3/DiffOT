{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# This needed to use dataloaders for some datasets\n",
    "from PIL import PngImagePlugin\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "LARGE_ENOUGH_NUMBER = 100\n",
    "PngImagePlugin.MAX_TEXT_CHUNK = LARGE_ENOUGH_NUMBER * (1024**2)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0xBADBEEF\n",
    "# GPU choosing\n",
    "DEVICE_IDS = [0]\n",
    "# For Colored MNIST exps\n",
    "DATASET1, DATASET1_PATH = \"MNIST-colored_2\", \"~/data/MNIST\"\n",
    "DATASET2, DATASET2_PATH = \"MNIST-colored_3\", \"~/data/MNIST\"\n",
    "DATASET1_CHANNELS = 3\n",
    "DATASET2_CHANNELS = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# change below 3 variables to get real EXP_NAME\n",
    "STRATEGY = \"Normal\"  # [Normal|Fix|Adapt]\n",
    "PIVOTAL_LIST = [50, 100]\n",
    "TIME = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "\n",
    "EXP_NAME = f\"{STRATEGY}_{DATASET1}_to_{DATASET2}_pivotal_{'_'.join(map(str, PIVOTAL_LIST))}_{TIME}\"\n",
    "LOAD_PATH = f\"../logs/{EXP_NAME}/\"\n",
    "\n",
    "print(f\"{EXP_NAME = }\")\n",
    "print(f\"{LOAD_PATH = }\")\n",
    "with open(os.path.join(LOAD_PATH, \"config.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    CONFIG = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = CONFIG.EPSILON\n",
    "\n",
    "N_STEPS = CONFIG.N_STEPS\n",
    "BATCH_SIZE = CONFIG.BATCH_SIZE\n",
    "\n",
    "PIVOTAL_LIST = CONFIG.PIVOTAL_LIST\n",
    "\n",
    "\n",
    "BETA_NET_LR, SDE_LR = CONFIG.BETA_NET_LR, CONFIG.SDE_LR\n",
    "BETA_NET_BETA, SDE_BETA = CONFIG.BETA_NET_BETA, CONFIG.SDE_BETA\n",
    "\n",
    "\n",
    "IMG_SIZE = CONFIG.IMG_SIZE\n",
    "UNET_BASE_FACTOR = CONFIG.UNET_BASE_FACTOR\n",
    "\n",
    "TIME_DIM = CONFIG.TIME_DIM\n",
    "USE_POSITIONAL_ENCODING = CONFIG.USE_POSITIONAL_ENCODING\n",
    "\n",
    "PREDICT_SHIFT = CONFIG.PREDICT_SHIFT\n",
    "\n",
    "USE_GRADIENT_CHECKPOINT = CONFIG.USE_GRADIENT_CHECKPOINT\n",
    "N_LAST_STEPS_WITHOUT_NOISE = CONFIG.N_LAST_STEPS_WITHOUT_NOISE\n",
    "IMAGE_INPUT = CONFIG.IMAGE_INPUT\n",
    "FID_EPOCHS = CONFIG.FID_EPOCHS\n",
    "\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "torch.cuda.set_device(f\"cuda:{DEVICE_IDS[0]}\")\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initalize data sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import load_dataset\n",
    "\n",
    "X_sampler, X_test_sampler = load_dataset(\n",
    "    DATASET1,\n",
    "    DATASET1_PATH,\n",
    "    img_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=1,\n",
    ")\n",
    "Y_sampler, Y_test_sampler = load_dataset(\n",
    "    DATASET2, DATASET2_PATH, img_size=IMG_SIZE, batch_size=BATCH_SIZE, num_workers=1\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "clear_output()\n",
    "\n",
    "logging.info(\"Initialize data sampler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import weights_init_D\n",
    "\n",
    "from src.cunet import CUNet\n",
    "from src.enot import SDE\n",
    "from src.resnet2 import ResNet_D\n",
    "\n",
    "SDEs, BETA_NETs = [], []\n",
    "SDE_OPTs, BETA_NET_OPTs = [], []\n",
    "SDE_SCHEDULERs, BETA_NET_SCHEDULERs = [], []\n",
    "\n",
    "for i in range(len(PIVOTAL_LIST) * 2 - 2):\n",
    "    sde = CUNet(\n",
    "        DATASET1_CHANNELS, DATASET2_CHANNELS, TIME_DIM, base_factor=UNET_BASE_FACTOR\n",
    "    ).cuda()\n",
    "\n",
    "    sde = SDE(\n",
    "        shift_model=sde,\n",
    "        epsilon=EPSILON,\n",
    "        n_steps=N_STEPS,\n",
    "        time_dim=TIME_DIM,\n",
    "        n_last_steps_without_noise=N_LAST_STEPS_WITHOUT_NOISE,\n",
    "        use_positional_encoding=USE_POSITIONAL_ENCODING,\n",
    "        use_gradient_checkpoint=USE_GRADIENT_CHECKPOINT,\n",
    "        predict_shift=PREDICT_SHIFT,\n",
    "        image_input=IMAGE_INPUT,\n",
    "    ).cuda()\n",
    "    SDEs.append(sde)\n",
    "\n",
    "    beta_net = ResNet_D(IMG_SIZE, nc=DATASET2_CHANNELS).cuda()\n",
    "    beta_net.apply(weights_init_D)\n",
    "    BETA_NETs.append(beta_net)\n",
    "\n",
    "    sde_opt = torch.optim.Adam(\n",
    "        sde.parameters(), lr=SDE_LR, weight_decay=1e-10, betas=(SDE_BETA, 0.999)\n",
    "    )\n",
    "    beta_net_opt = torch.optim.Adam(\n",
    "        beta_net.parameters(),\n",
    "        lr=BETA_NET_LR,\n",
    "        weight_decay=1e-10,\n",
    "        betas=(BETA_NET_BETA, 0.999),\n",
    "    )\n",
    "    SDE_OPTs.append(sde_opt)\n",
    "    BETA_NET_OPTs.append(beta_net_opt)\n",
    "\n",
    "    sde_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        sde_opt, milestones=[15000, 25000, 40000, 55000, 70000], gamma=0.5\n",
    "    )\n",
    "    beta_net_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        beta_net_opt, milestones=[15000, 25000, 40000, 55000, 70000], gamma=0.5\n",
    "    )\n",
    "    SDE_SCHEDULERs.append(sde_scheduler)\n",
    "    BETA_NET_SCHEDULERs.append(beta_net_scheduler)\n",
    "\n",
    "\n",
    "if len(DEVICE_IDS) > 1:\n",
    "    for i in range(len(SDEs)):\n",
    "        SDEs[i] = nn.DataParallel(SDEs[i], device_ids=DEVICE_IDS)\n",
    "        BETA_NETs[i] = nn.DataParallel(BETA_NETs[i], device_ids=DEVICE_IDS)\n",
    "\n",
    "        print(\"T params:\", np.sum([np.prod(p.shape) for p in SDEs[0].parameters()]))\n",
    "        print(\n",
    "            \"D params:\", np.sum([np.prod(p.shape) for p in BETA_NETs[0].parameters()])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_DIR = os.path.join(LOAD_PATH, \"iter2000/\")\n",
    "for i, sde in enumerate(SDEs):\n",
    "    path = os.path.join(CKPT_DIR, f\"sde{i}.pt\")\n",
    "    print(f\"{path = }\")\n",
    "    sde.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trans example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import distributions\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def draw_linked_mapping(\n",
    "    source_sampler: distributions.Sampler,\n",
    "    target_sampler: distributions.Sampler,\n",
    "    SDEs: list[SDE],\n",
    "    plot_n_samples: int = 4,\n",
    "):\n",
    "    device = next(SDEs[0].parameters()).device\n",
    "    source_dataset, target_dataset, mapped_dataset = [], [], []\n",
    "    pivotals_list = []\n",
    "    for i in range(plot_n_samples):\n",
    "        source, target = (\n",
    "            source_sampler.sample(1).to(device),\n",
    "            target_sampler.sample(1).to(device),\n",
    "        )\n",
    "\n",
    "        pivotals = [source.clone().detach()]\n",
    "        for t in range(len(SDEs)):\n",
    "            x0 = pivotals[t]\n",
    "            trajectory, times, _ = SDEs[t](x0)\n",
    "            xN = trajectory[:, -1, :]\n",
    "            pivotals.append(xN)\n",
    "\n",
    "        mapped_dataset.append(pivotals[-1])\n",
    "        source_dataset.append(source)\n",
    "        target_dataset.append(target)\n",
    "\n",
    "        pivotals.append(target.clone().detach())\n",
    "        pivotals_list.append(pivotals)\n",
    "\n",
    "        fig, axes = plt.subplots(1, len(SDEs) + 2, figsize=(5 * (len(SDEs) + 2), 5))\n",
    "        axes = axes.flatten()\n",
    "        for pivotal, ax in zip(pivotals, axes):\n",
    "            img = torch.squeeze(pivotal).to(\"cpu\")\n",
    "            img = img.permute(1, 2, 0).mul(0.5).add(0.5).numpy().clip(0, 1)\n",
    "            ax.imshow(img)\n",
    "\n",
    "        fig.tight_layout(pad=0.001)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return pivotals_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivotals_list shape (plot_n_samples, n_steps, (RGB image))\n",
    "pivotals_list = draw_linked_mapping(\n",
    "    X_sampler,\n",
    "    Y_sampler,\n",
    "    SDEs,\n",
    "    plot_n_samples=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pivotals in pivotals_list:\n",
    "    fig, axes = plt.subplots(1, 8, figsize=(40, 5))\n",
    "    axes = axes.flatten()\n",
    "    for pivotal, ax in zip(pivotals, axes):\n",
    "        img = torch.squeeze(pivotal).to(\"cpu\")\n",
    "        img = img.permute(1, 2, 0).mul(0.5).add(0.5).numpy().clip(0, 1)\n",
    "        ax.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load FID stats\n",
    "\n",
    "should run ../../stats/compute_stats.ipynb to get stats json file first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../stats/{}_{}_test.json\".format(DATASET2, IMG_SIZE)\n",
    "with open(filename, \"r\") as fp:\n",
    "    data_stats = json.load(fp)\n",
    "    mu_data, sigma_data = data_stats[\"mu\"], data_stats[\"sigma\"]\n",
    "del data_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate FID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fid_score import calculate_frechet_distance\n",
    "from tools import get_linked_sdes_pushed_loader_stats\n",
    "\n",
    "print(\"Computing FID\")\n",
    "mu, sigma = get_linked_sdes_pushed_loader_stats(\n",
    "    SDEs, X_test_sampler.loader, n_epochs=FID_EPOCHS, batch_size=BATCH_SIZE\n",
    ")\n",
    "fid = calculate_frechet_distance(mu_data, sigma_data, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACC\n",
    "\n",
    "TODO:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
