{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments Gaussian optimal Transport in High Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from scipy.linalg import inv, sqrtm\n",
    "from scipy.stats import ortho_group\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src import distributions\n",
    "\n",
    "# from src.icnn import DenseICNN\n",
    "# from src.tools import compute_l1_norm, ewma\n",
    "from src.fid_score import calculate_frechet_distance\n",
    "from src.tools import freeze, unfreeze\n",
    "\n",
    "torch.random.manual_seed(0xBADBEEF)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 2\n",
    "assert DIM > 1\n",
    "\n",
    "OUTPUT_SEED = 0xC0FFEE\n",
    "L1 = 1e-10\n",
    "GPU_DEVICE = 0\n",
    "BATCH_SIZE = 512\n",
    "EPSILON = 10\n",
    "N_STEPS = 50\n",
    "TIME_DIM = 1\n",
    "T_LR = 3e-4\n",
    "D_LR = 3e-4\n",
    "T_ITERS = 10\n",
    "CONSTANT_TIME = False\n",
    "USE_POSITIONAL_ENCODING = False\n",
    "INTEGRAL_SCALE = 1 / (DIM)\n",
    "T_GRADIENT_MAX_NORM = float(\"inf\")\n",
    "D_GRADIENT_MAX_NORM = float(\"inf\")\n",
    "IS_RESNET_GENERATOR = False\n",
    "PREDICT_SHIFT = True\n",
    "N_LAST_STEPS_WITHOUT_NOISE = 1\n",
    "\n",
    "T_N_HIDDEN = 512\n",
    "T_N_LAYERS = 3\n",
    "\n",
    "D_N_HIDDEN = 512\n",
    "D_N_LAYERS = 3\n",
    "\n",
    "MAX_STEPS = 10000\n",
    "CONTINUE = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = f\"Gaussians_test_EPSILON_{EPSILON}_STEPS_{N_STEPS}_DIM_{DIM}\"\n",
    "\n",
    "config = dict(\n",
    "    DIM=DIM,\n",
    "    T_ITERS=T_ITERS,\n",
    "    D_LR=D_LR,\n",
    "    T_LR=T_LR,\n",
    "    BATCH_SIZE=BATCH_SIZE,\n",
    "    N_STEPS=N_STEPS,\n",
    "    EPSILON=EPSILON,\n",
    "    CONSTANT_TIME=CONSTANT_TIME,\n",
    "    USE_POSITIONAL_ENCODING=USE_POSITIONAL_ENCODING,\n",
    "    TIME_DIM=TIME_DIM,\n",
    "    INTEGRAL_SCALE=INTEGRAL_SCALE,\n",
    "    T_GRADIENT_MAX_NORM=T_GRADIENT_MAX_NORM,\n",
    "    D_GRADIENT_MAX_NORM=D_GRADIENT_MAX_NORM,\n",
    "    IS_RESNET_GENERATOR=IS_RESNET_GENERATOR,\n",
    "    T_N_HIDDEN=T_N_HIDDEN,\n",
    "    T_N_LAYERS=T_N_LAYERS,\n",
    "    D_N_HIDDEN=D_N_HIDDEN,\n",
    "    D_N_LAYERS=D_N_LAYERS,\n",
    "    PREDICT_SHIFT=PREDICT_SHIFT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "torch.cuda.set_device(GPU_DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Wasserstein-2 Distance:  1.0355121796910387\n",
      "Variance of X: 4.2659435\n",
      "Variance of Y: 4.267611\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(OUTPUT_SEED)\n",
    "torch.manual_seed(OUTPUT_SEED)\n",
    "\n",
    "mu_0 = np.zeros(DIM)\n",
    "mu_T = np.zeros(DIM)\n",
    "mu_optimal_plan = np.zeros(2 * DIM)\n",
    "\n",
    "rotation_Y = ortho_group.rvs(DIM)\n",
    "weight_Y = rotation_Y @ np.diag(np.exp(np.linspace(np.log(0.5), np.log(2), DIM)))\n",
    "sigma_Y = weight_Y @ weight_Y.T\n",
    "Y_sampler = distributions.LinearTransformer(\n",
    "    distributions.StandartNormalSampler(dim=DIM), weight_Y, bias=None\n",
    ")\n",
    "\n",
    "rotation_X = ortho_group.rvs(DIM)\n",
    "weight_X = rotation_X @ np.diag(np.exp(np.linspace(np.log(0.5), np.log(2), DIM)))\n",
    "sigma_X = weight_X @ weight_X.T\n",
    "X_sampler = distributions.LinearTransformer(\n",
    "    distributions.StandartNormalSampler(dim=DIM), weight_X, bias=None\n",
    ")\n",
    "\n",
    "BW = calculate_frechet_distance(np.zeros(DIM), sigma_X, np.zeros(DIM), sigma_Y) / 2\n",
    "print(\"True Wasserstein-2 Distance: \", BW)\n",
    "\n",
    "X = X_sampler.sample(100000).cpu().detach().numpy()\n",
    "Var_X = np.sum(np.var(X, axis=0))\n",
    "print(\"Variance of X:\", Var_X)\n",
    "\n",
    "Y = Y_sampler.sample(100000).cpu().detach().numpy()\n",
    "Var_Y = np.sum(np.var(Y, axis=0))\n",
    "print(\"Variance of Y:\", np.sum(Var_Y))\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Functions for calculating BW-UVP metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetrize(X):\n",
    "    return np.real((X + X.T) / 2)\n",
    "\n",
    "\n",
    "def get_D_sigma(covariance_0, covariance_T, epsilon):\n",
    "    shape = covariance_0.shape[0]\n",
    "\n",
    "    covariance_0_sqrt = symmetrize(sqrtm(covariance_0))\n",
    "    return symmetrize(\n",
    "        sqrtm(\n",
    "            4 * covariance_0_sqrt @ covariance_T @ covariance_0_sqrt\n",
    "            + (epsilon**2) * np.eye(shape)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def get_C_sigma(covariance_0, D_sigma, epsilon):\n",
    "    shape = covariance_0.shape[0]\n",
    "\n",
    "    covariance_0_sqrt = symmetrize(sqrtm(covariance_0))\n",
    "    covariance_0_sqrt_inv = inv(covariance_0_sqrt)\n",
    "\n",
    "    return 0.5 * (\n",
    "        covariance_0_sqrt @ D_sigma @ covariance_0_sqrt_inv - epsilon * np.eye(shape)\n",
    "    )\n",
    "\n",
    "\n",
    "def get_mu_t(t, mu_0, mu_T):\n",
    "    return (1 - t) * mu_0 + t * mu_T\n",
    "\n",
    "\n",
    "def get_covariance_t(t, covariance_0, covariance_T, C_sigma, epsilon):\n",
    "    shape = covariance_0.shape[0]\n",
    "\n",
    "    return (\n",
    "        ((1 - t) ** 2) * covariance_0\n",
    "        + (t**2) * covariance_T\n",
    "        + t * (1 - t) * (C_sigma + C_sigma.T)\n",
    "        + epsilon * t * (1 - t) * np.eye(shape)\n",
    "    )\n",
    "\n",
    "\n",
    "def get_conditional_covariance_t(t, covariance_0, covariance_T, C_sigma, epsilon):\n",
    "    shape = covariance_0.shape[0]\n",
    "\n",
    "    covariance_0_inv = inv(covariance_0)\n",
    "\n",
    "    return (t**2) * (\n",
    "        covariance_T - C_sigma.T @ covariance_0_inv @ C_sigma\n",
    "    ) + epsilon * t * (1 - t) * np.eye(shape)\n",
    "\n",
    "\n",
    "def get_conditional_mu_t(x0, mu_0, mu_T, t, covariance_0, C_sigma, epsilon):\n",
    "    shape = covariance_0.shape[0]\n",
    "\n",
    "    covariance_0_inv = inv(covariance_0)\n",
    "\n",
    "    return (1 - t) * x0 + t * (\n",
    "        mu_T + C_sigma.T @ covariance_0_inv @ (x0[:, None] - mu_0[:, None])\n",
    "    )\n",
    "\n",
    "\n",
    "def get_optimal_plan_covariance(covariance_0, covariance_T, C_sigma):\n",
    "    size = covariance_0.shape[0]\n",
    "    optimal_plan_covariance = np.zeros((2 * size, 2 * size))\n",
    "\n",
    "    optimal_plan_covariance[:size, :size] = covariance_0\n",
    "    optimal_plan_covariance[size:, size:] = covariance_T\n",
    "\n",
    "    optimal_plan_covariance[:size, size:] = C_sigma\n",
    "    optimal_plan_covariance[size:, :size] = C_sigma.T\n",
    "\n",
    "    return optimal_plan_covariance\n",
    "\n",
    "\n",
    "def compute_BW_UVP(samples, true_mu, true_covariance):\n",
    "    samples_covariance = np.cov(samples.T)\n",
    "    samples_mu = samples.mean(axis=0)\n",
    "    samples_covariance_sqrt = symmetrize(sqrtm(samples_covariance))\n",
    "\n",
    "    mu_term = 0.5 * ((true_mu - samples_mu) ** 2).sum()\n",
    "    covariance_term = (\n",
    "        0.5 * np.trace(samples_covariance)\n",
    "        + 0.5 * np.trace(true_covariance)\n",
    "        - np.trace(\n",
    "            symmetrize(\n",
    "                sqrtm(\n",
    "                    samples_covariance_sqrt @ true_covariance @ samples_covariance_sqrt\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    BW = mu_term + covariance_term\n",
    "    BW_UVP = 100 * (BW / (0.5 * np.trace(true_covariance)))\n",
    "\n",
    "    return BW_UVP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of parameters for BW-UVP metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_0 = sigma_X\n",
    "covariance_T = sigma_Y\n",
    "\n",
    "D_sigma = get_D_sigma(covariance_0, covariance_T, EPSILON)\n",
    "C_sigma = get_C_sigma(covariance_0, D_sigma, EPSILON)\n",
    "\n",
    "mu_t = np.stack(\n",
    "    [get_mu_t(t, mu_0, mu_T) for t in np.linspace(0, 1, N_STEPS + 1)], axis=0\n",
    ")\n",
    "\n",
    "covariance_t = np.stack(\n",
    "    [\n",
    "        get_covariance_t(t, covariance_0, covariance_T, C_sigma, EPSILON)\n",
    "        for t in np.linspace(0, 1, N_STEPS + 1)\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "optimal_plan_covariance = get_optimal_plan_covariance(\n",
    "    covariance_0, covariance_T, C_sigma\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Functions for the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.silu(input)\n",
    "\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim, scale):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.scale = scale\n",
    "\n",
    "        inv_freq = torch.exp(\n",
    "            torch.arange(0, dim, 2, dtype=torch.float32) * (-math.log(10000) / dim)\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "    def forward(self, input):\n",
    "        shape = input.shape\n",
    "\n",
    "        input = input * self.scale + 1\n",
    "        sinusoid_in = torch.ger(input.view(-1).float(), self.inv_freq)\n",
    "        pos_emb = torch.cat([sinusoid_in.sin(), sinusoid_in.cos()], dim=-1)\n",
    "        pos_emb = pos_emb.view(*shape, self.dim)\n",
    "\n",
    "        return pos_emb\n",
    "\n",
    "\n",
    "def make_net(n_inputs, n_outputs, n_layers=3, n_hiddens=100):\n",
    "    layers = [nn.Linear(n_inputs, n_hiddens), nn.ReLU()]\n",
    "\n",
    "    for i in range(n_layers - 1):\n",
    "        layers.extend([nn.Linear(n_hiddens, n_hiddens), nn.ReLU()])\n",
    "\n",
    "    layers.append(nn.Linear(n_hiddens, n_outputs))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class SDE(nn.Module):\n",
    "    def __init__(self, shift_model, epsilon, n_steps, time_dim, is_resnet_generator):\n",
    "        super().__init__()\n",
    "        self.shift_model = shift_model\n",
    "        self.epsilon = epsilon\n",
    "        self.n_steps = n_steps\n",
    "        self.delta_t = 1 / n_steps\n",
    "        self.is_resnet_generator = is_resnet_generator\n",
    "\n",
    "        self.time = nn.Sequential(\n",
    "            TimeEmbedding(time_dim, scale=n_steps),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x0):\n",
    "        if self.is_resnet_generator:\n",
    "            trajectory, shifts = self.shift_model(x0)\n",
    "            times = torch.linspace(0, 1, self.n_steps + 1)\n",
    "\n",
    "            return trajectory, times, shifts\n",
    "        else:\n",
    "            t0 = 0.0\n",
    "            trajectory = [x0]\n",
    "            times = [t0]\n",
    "            shifts = []\n",
    "\n",
    "            x, t = x0, t0\n",
    "\n",
    "            for step in range(self.n_steps):\n",
    "                x, t, shift = self._step(x, t)\n",
    "\n",
    "                trajectory.append(x)\n",
    "                times.append(t)\n",
    "                shifts.append(shift)\n",
    "\n",
    "            return (\n",
    "                torch.stack(trajectory, dim=1),\n",
    "                torch.tensor(times),\n",
    "                torch.stack(shifts, dim=1),\n",
    "            )\n",
    "\n",
    "    def _step(self, x, t):\n",
    "        if PREDICT_SHIFT:\n",
    "            shift = self._get_shift(x, t)\n",
    "            shifted_x = x + shift * torch.tensor(self.delta_t).cuda()\n",
    "        else:\n",
    "            shifted_x = self._get_shift(x, t)\n",
    "            shift = (shifted_x - x) / (torch.tensor(self.delta_t).cuda())\n",
    "        noise = self._sample_noise(x)\n",
    "\n",
    "        return shifted_x + noise, t + self.delta_t, shift\n",
    "\n",
    "    def _get_shift(self, x, t):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        if CONSTANT_TIME:\n",
    "            t = 0.0\n",
    "\n",
    "        if USE_POSITIONAL_ENCODING:\n",
    "            t = torch.tensor(t).repeat(batch_size)\n",
    "            t = t.cuda()\n",
    "            t = self.time(t)\n",
    "        else:\n",
    "            t = torch.tensor(t).repeat(batch_size)[:, None]\n",
    "            if x.device.type == \"cuda\":\n",
    "                t = t.cuda()\n",
    "\n",
    "        x_t = torch.cat((x, t), dim=-1)\n",
    "\n",
    "        return self.shift_model(x_t)\n",
    "\n",
    "    def _sample_noise(self, x):\n",
    "        noise = math.sqrt(self.epsilon) * math.sqrt(self.delta_t) * torch.randn(x.shape)\n",
    "\n",
    "        if x.device.type == \"cuda\":\n",
    "            noise = noise.cuda()\n",
    "        return noise\n",
    "\n",
    "    def set_n_steps(self, n_steps):\n",
    "        self.n_steps = n_steps\n",
    "        self.delta_t = 1 / n_steps\n",
    "\n",
    "\n",
    "def integrate(values, times):\n",
    "    deltas = times[1:] - times[:-1]\n",
    "    if values.device.type == \"cuda\":\n",
    "        deltas = deltas.cuda()\n",
    "    return (values * deltas[None, :]).sum(dim=1)\n",
    "\n",
    "\n",
    "def compute_metrics(X_sampler, T, N_STEPS, mu_t, covariance_t):\n",
    "    X = X_sampler.sample(100000)\n",
    "\n",
    "    trajectory = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(100000 // BATCH_SIZE + 1):\n",
    "            trajectory.append(\n",
    "                T(X[BATCH_SIZE * i : BATCH_SIZE * (i + 1)])[0].cpu().numpy()\n",
    "            )\n",
    "\n",
    "    trajectory = np.concatenate(trajectory, axis=0)\n",
    "\n",
    "    result = []\n",
    "    for step in range(1, N_STEPS + 1):\n",
    "        T_X = trajectory[:, step, :]\n",
    "\n",
    "        T_X_covariance = np.cov(T_X.T)\n",
    "        T_X_mu = T_X.mean(axis=0)\n",
    "\n",
    "        true_mu = mu_t[step]\n",
    "        true_covariance = covariance_t[step]\n",
    "\n",
    "        T_X_covariance_sqrt = symmetrize(sqrtm(T_X_covariance))\n",
    "\n",
    "        mu_term = 0.5 * ((true_mu - T_X_mu) ** 2).sum()\n",
    "        covariance_term = (\n",
    "            0.5 * np.trace(T_X_covariance)\n",
    "            + 0.5 * np.trace(true_covariance)\n",
    "            - np.trace(\n",
    "                symmetrize(\n",
    "                    sqrtm(T_X_covariance_sqrt @ true_covariance @ T_X_covariance_sqrt)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        BW = mu_term + covariance_term\n",
    "        BW_UVP = 100 * (BW / (0.5 * np.trace(true_covariance)))\n",
    "\n",
    "        result.append(BW_UVP)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def calculate_noise_norm(dim, n_steps, epsilon):\n",
    "    n = dim\n",
    "    dt = 1 / n_steps\n",
    "    sigma = math.sqrt(dt) * math.sqrt(epsilon)\n",
    "\n",
    "    return n_steps * (\n",
    "        math.exp(\n",
    "            math.log(sigma)\n",
    "            + math.log(math.sqrt(2))\n",
    "            + math.lgamma((n + 1) / 2)\n",
    "            - math.lgamma(n / 2)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T params: 528390\n",
      "D params: 527361\n"
     ]
    }
   ],
   "source": [
    "D = make_net(DIM, 1, n_layers=D_N_LAYERS, n_hiddens=D_N_HIDDEN).cuda()\n",
    "\n",
    "T = make_net(DIM + TIME_DIM, DIM, n_layers=T_N_LAYERS, n_hiddens=T_N_HIDDEN).cuda()\n",
    "T = SDE(T, EPSILON, N_STEPS, TIME_DIM, IS_RESNET_GENERATOR).cuda()\n",
    "\n",
    "print(\"T params:\", np.sum([np.prod(p.shape) for p in T.parameters()]))\n",
    "print(\"D params:\", np.sum([np.prod(p.shape) for p in D.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_opt = torch.optim.Adam(T.parameters(), lr=T_LR, weight_decay=1e-10)\n",
    "D_opt = torch.optim.Adam(D.parameters(), lr=D_LR, weight_decay=1e-10)\n",
    "\n",
    "if CONTINUE > -1:\n",
    "    T_opt.load_state_dict(\n",
    "        torch.load(os.path.join(OUTPUT_PATH, f\"T_opt_{SEED}_{CONTINUE}.pt\"))\n",
    "    )\n",
    "    T.load_state_dict(torch.load(os.path.join(OUTPUT_PATH, f\"T_{SEED}_{CONTINUE}.pt\")))\n",
    "    D_opt.load_state_dict(\n",
    "        torch.load(os.path.join(OUTPUT_PATH, f\"D_opt_{SEED}_{CONTINUE}.pt\"))\n",
    "    )\n",
    "    D.load_state_dict(torch.load(os.path.join(OUTPUT_PATH, f\"D_{SEED}_{CONTINUE}.pt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/zyz/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/zyz/ENOT/notebooks/wandb/run-20240422_213901-9rgc5d1n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vve/ENOT-notebooks/runs/9rgc5d1n' target=\"_blank\">Gaussians_test_EPSILON_10_STEPS_50_DIM_2</a></strong> to <a href='https://wandb.ai/vve/ENOT-notebooks' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vve/ENOT-notebooks' target=\"_blank\">https://wandb.ai/vve/ENOT-notebooks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vve/ENOT-notebooks/runs/9rgc5d1n' target=\"_blank\">https://wandb.ai/vve/ENOT-notebooks/runs/9rgc5d1n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/vve/ENOT-notebooks/runs/9rgc5d1n?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa01329cd70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(name=EXP_NAME, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1008/10000 [18:05<2:41:22,  1.08s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m integral \u001b[38;5;241m=\u001b[39m INTEGRAL_SCALE \u001b[38;5;241m*\u001b[39m integrate(norm, times)\n\u001b[1;32m     20\u001b[0m T_loss \u001b[38;5;241m=\u001b[39m (integral \u001b[38;5;241m+\u001b[39m D(X1) \u001b[38;5;241m-\u001b[39m D(XN))\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m---> 21\u001b[0m \u001b[43mT_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m T_gradient_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m     23\u001b[0m     T\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39mT_GRADIENT_MAX_NORM\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m T_opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/ENOT/.venv/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ENOT/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "\n",
    "noise_norm = calculate_noise_norm(DIM, N_STEPS, EPSILON)\n",
    "wandb.log({\"Noise norm\": noise_norm}, step=0)\n",
    "\n",
    "for step in tqdm(range(CONTINUE + 1, MAX_STEPS)):\n",
    "    unfreeze(T)\n",
    "    freeze(D)\n",
    "    for t_iter in range(T_ITERS):\n",
    "        T_opt.zero_grad()\n",
    "\n",
    "        X0, X1 = X_sampler.sample(BATCH_SIZE), Y_sampler.sample(BATCH_SIZE)\n",
    "        X0.requires_grad_()\n",
    "\n",
    "        trajectory, times, shifts = T(X0)\n",
    "        XN = trajectory[:, -1]\n",
    "        norm = torch.norm(shifts.flatten(start_dim=2), p=2, dim=-1) ** 2\n",
    "        integral = INTEGRAL_SCALE * integrate(norm, times)\n",
    "\n",
    "        T_loss = (integral + D(X1) - D(XN)).mean()\n",
    "        T_loss.backward()\n",
    "        T_gradient_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            T.parameters(), max_norm=T_GRADIENT_MAX_NORM\n",
    "        )\n",
    "        T_opt.step()\n",
    "\n",
    "    wandb.log({\"T gradient norm\": T_gradient_norm.item()}, step=step)\n",
    "    wandb.log({\"Mean norm\": torch.sqrt(norm).mean().item()}, step=step)\n",
    "    wandb.log({\"T_loss\": T_loss.item()}, step=step)\n",
    "\n",
    "    del T_loss, X0, X1, XN\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    freeze(T)\n",
    "    unfreeze(D)\n",
    "\n",
    "    D_opt.zero_grad()\n",
    "\n",
    "    X0, X1 = X_sampler.sample(BATCH_SIZE), Y_sampler.sample(BATCH_SIZE)\n",
    "    trajectory, times, shifts = T(X0)\n",
    "    XN = trajectory[:, -1]\n",
    "    norm = torch.norm(shifts.flatten(start_dim=2), p=2, dim=-1) ** 2\n",
    "    integral = INTEGRAL_SCALE * integrate(norm, times)\n",
    "\n",
    "    D_X1 = D(X1)\n",
    "    D_XN = D(XN)\n",
    "\n",
    "    D_loss = (-integral - D_X1 + D_XN).mean()\n",
    "    D_loss.backward()\n",
    "    D_gradient_norm = torch.nn.utils.clip_grad_norm_(\n",
    "        D.parameters(), max_norm=D_GRADIENT_MAX_NORM\n",
    "    )\n",
    "    D_opt.step()\n",
    "\n",
    "    wandb.log({\"D gradient norm\": D_gradient_norm.item()}, step=step)\n",
    "    wandb.log({\"D_loss\": D_loss.item()}, step=step)\n",
    "\n",
    "    wandb.log({\"integral\": integral.mean().item()}, step=step)\n",
    "    wandb.log({\"D_X1\": D_X1.mean().item()}, step=step)\n",
    "    wandb.log({\"D_XN\": D_XN.mean().item()}, step=step)\n",
    "    del D_loss, X0, X1, XN\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        metrics.append(compute_metrics(X_sampler, T, N_STEPS, mu_t, covariance_t))\n",
    "\n",
    "        for i in range(N_STEPS):\n",
    "            wandb.log({f\"BW_UVP_{i}\": metrics[-1][i]}, step=step)\n",
    "\n",
    "        wandb.log({\"BW_UVP_mean\": np.mean(metrics[-1])}, step=step)\n",
    "        wandb.log({\"BW_UVP_max\": max(metrics[-1])}, step=step)\n",
    "\n",
    "        X = X_sampler.sample(100000)\n",
    "\n",
    "        trajectory = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(100000 // BATCH_SIZE + 1):\n",
    "                trajectory.append(\n",
    "                    T(X[BATCH_SIZE * i : BATCH_SIZE * (i + 1)])[0].cpu().numpy()\n",
    "                )\n",
    "\n",
    "        trajectory = np.concatenate(trajectory, axis=0)\n",
    "\n",
    "        TX = trajectory[:, -1]\n",
    "        X = X.cpu().numpy()\n",
    "        X_TX = np.concatenate((X, TX), axis=1)\n",
    "\n",
    "        BW_UVP_target_distr = compute_BW_UVP(TX, mu_T, covariance_T)\n",
    "        wandb.log({\"BW_UVP_target_distr\": BW_UVP_target_distr}, step=step)\n",
    "\n",
    "        BW_UVP_optimal_plan = compute_BW_UVP(\n",
    "            X_TX, mu_optimal_plan, optimal_plan_covariance\n",
    "        )\n",
    "        wandb.log({\"BW_UVP_optimal_plan\": BW_UVP_optimal_plan}, step=step)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bfe988de3dc431a8dd99636d7484b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>BW_UVP_0</td><td>█▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>BW_UVP_1</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_10</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_11</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_12</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_13</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_14</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_15</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_16</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_17</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_18</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_19</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_2</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_20</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_21</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_22</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_23</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_24</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_25</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_26</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_27</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_28</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_29</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_3</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_30</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_31</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_32</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_33</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_34</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_35</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_36</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_37</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_38</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_39</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_4</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_40</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_41</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_42</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_43</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_44</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_45</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_46</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_47</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_48</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_49</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_5</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_6</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_7</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_8</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_9</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_max</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_mean</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_optimal_plan</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BW_UVP_target_distr</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>D gradient norm</td><td>▅▄▇█▄█▄▅▄▂▄▄▃▅▄▆▂▂▂▂▁▄▄▄▂▄▅▄▅▅▆▄▁▁▃▂▂▆▄▃</td></tr><tr><td>D_X1</td><td>█▄▄▃▂▁▁▂▂▂▃▂▂▂▂▁▂▂▃▃▃▃▂▃▃▂▂▂▃▃▃▂▂▂▃▃▂▃▂▂</td></tr><tr><td>D_XN</td><td>█▄▃▂▂▁▁▂▃▂▂▂▂▃▂▂▂▂▃▃▃▃▃▃▃▂▂▂▃▂▂▂▂▃▃▃▂▂▃▂</td></tr><tr><td>D_loss</td><td>█▄▂▂▁▂▂▂▂▂▁▂▂▃▂▃▁▂▂▁▂▂▃▂▂▁▂▃▁▁▁▁▂▂▂▂▁▁▂▂</td></tr><tr><td>Mean norm</td><td>▁▇▇▇████▇███▇████████▇██████▇▇██▇██▇████</td></tr><tr><td>Noise norm</td><td>▁</td></tr><tr><td>T gradient norm</td><td>▁▁▂▃▅█▄▄▆▇▅▅▆▅▆▆▅▃▇▆▃▃▄▅▂▅█▄▄▄▄▇▅▅█▃▅▄▄▇</td></tr><tr><td>T_loss</td><td>▁▅▇▇█▇▇▇█▇█▇▇▇▇▇▇▆█▇▇▇▇▇▇▇█▇█▆▇▇▇▇▇▆█▇▇▇</td></tr><tr><td>integral</td><td>▁▄▆▇█▇█▇█▇█▇▇▇▇██▇▇█▇▇▇█▇██▇▇▇▇▇▇▇█▇▇▇█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>BW_UVP_0</td><td>0.0054</td></tr><tr><td>BW_UVP_1</td><td>0.00854</td></tr><tr><td>BW_UVP_10</td><td>0.06112</td></tr><tr><td>BW_UVP_11</td><td>0.06331</td></tr><tr><td>BW_UVP_12</td><td>0.06465</td></tr><tr><td>BW_UVP_13</td><td>0.07629</td></tr><tr><td>BW_UVP_14</td><td>0.0783</td></tr><tr><td>BW_UVP_15</td><td>0.08824</td></tr><tr><td>BW_UVP_16</td><td>0.09573</td></tr><tr><td>BW_UVP_17</td><td>0.10076</td></tr><tr><td>BW_UVP_18</td><td>0.10953</td></tr><tr><td>BW_UVP_19</td><td>0.11517</td></tr><tr><td>BW_UVP_2</td><td>0.01291</td></tr><tr><td>BW_UVP_20</td><td>0.11555</td></tr><tr><td>BW_UVP_21</td><td>0.11865</td></tr><tr><td>BW_UVP_22</td><td>0.12433</td></tr><tr><td>BW_UVP_23</td><td>0.13239</td></tr><tr><td>BW_UVP_24</td><td>0.137</td></tr><tr><td>BW_UVP_25</td><td>0.14161</td></tr><tr><td>BW_UVP_26</td><td>0.1429</td></tr><tr><td>BW_UVP_27</td><td>0.14259</td></tr><tr><td>BW_UVP_28</td><td>0.15238</td></tr><tr><td>BW_UVP_29</td><td>0.15337</td></tr><tr><td>BW_UVP_3</td><td>0.01456</td></tr><tr><td>BW_UVP_30</td><td>0.15239</td></tr><tr><td>BW_UVP_31</td><td>0.15944</td></tr><tr><td>BW_UVP_32</td><td>0.16579</td></tr><tr><td>BW_UVP_33</td><td>0.17033</td></tr><tr><td>BW_UVP_34</td><td>0.17683</td></tr><tr><td>BW_UVP_35</td><td>0.17521</td></tr><tr><td>BW_UVP_36</td><td>0.17847</td></tr><tr><td>BW_UVP_37</td><td>0.18526</td></tr><tr><td>BW_UVP_38</td><td>0.18799</td></tr><tr><td>BW_UVP_39</td><td>0.19077</td></tr><tr><td>BW_UVP_4</td><td>0.01975</td></tr><tr><td>BW_UVP_40</td><td>0.18954</td></tr><tr><td>BW_UVP_41</td><td>0.18808</td></tr><tr><td>BW_UVP_42</td><td>0.18174</td></tr><tr><td>BW_UVP_43</td><td>0.18486</td></tr><tr><td>BW_UVP_44</td><td>0.18543</td></tr><tr><td>BW_UVP_45</td><td>0.17793</td></tr><tr><td>BW_UVP_46</td><td>0.18906</td></tr><tr><td>BW_UVP_47</td><td>0.18976</td></tr><tr><td>BW_UVP_48</td><td>0.19457</td></tr><tr><td>BW_UVP_49</td><td>0.21726</td></tr><tr><td>BW_UVP_5</td><td>0.02377</td></tr><tr><td>BW_UVP_6</td><td>0.0325</td></tr><tr><td>BW_UVP_7</td><td>0.04207</td></tr><tr><td>BW_UVP_8</td><td>0.04543</td></tr><tr><td>BW_UVP_9</td><td>0.05116</td></tr><tr><td>BW_UVP_max</td><td>0.21726</td></tr><tr><td>BW_UVP_mean</td><td>0.12221</td></tr><tr><td>BW_UVP_optimal_plan</td><td>0.13864</td></tr><tr><td>BW_UVP_target_distr</td><td>0.26343</td></tr><tr><td>D gradient norm</td><td>4.02935</td></tr><tr><td>D_X1</td><td>-27.78493</td></tr><tr><td>D_XN</td><td>-28.60938</td></tr><tr><td>D_loss</td><td>-20.27441</td></tr><tr><td>Mean norm</td><td>4.4363</td></tr><tr><td>Noise norm</td><td>28.02496</td></tr><tr><td>T gradient norm</td><td>46.08905</td></tr><tr><td>T_loss</td><td>19.43948</td></tr><tr><td>integral</td><td>19.44996</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Gaussians_test_EPSILON_10_STEPS_50_DIM_2</strong> at: <a href='https://wandb.ai/vve/ENOT-notebooks/runs/9rgc5d1n' target=\"_blank\">https://wandb.ai/vve/ENOT-notebooks/runs/9rgc5d1n</a><br/> View project at: <a href='https://wandb.ai/vve/ENOT-notebooks' target=\"_blank\">https://wandb.ai/vve/ENOT-notebooks</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240422_213901-9rgc5d1n/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
