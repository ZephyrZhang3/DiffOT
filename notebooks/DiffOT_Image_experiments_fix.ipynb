{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Code for experiments with colored MNIST and Celeba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T05:41:05.232831Z",
     "iopub.status.busy": "2024-05-26T05:41:05.231993Z",
     "iopub.status.idle": "2024-05-26T05:41:07.193032Z",
     "shell.execute_reply": "2024-05-26T05:41:07.192182Z",
     "shell.execute_reply.started": "2024-05-26T05:41:05.232763Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from diffusers import DDIMScheduler\n",
    "from torchvision.transforms import Compose, Resize, Normalize, ToTensor, Lambda\n",
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n",
    "\n",
    "from src.mnistm_utils import MNISTM\n",
    "from src.guided_samplers import PairedSubsetSampler, SubsetGuidedDataset, get_indicies_subset\n",
    "\n",
    "from src.cunet import CUNet\n",
    "from src.enot import SDE, integrate\n",
    "from src.resnet2 import ResNet_D\n",
    "\n",
    "from src.new_plotters import plot_fixed_linked_sdes_images, plot_random_linked_sdes_images\n",
    "\n",
    "from src.tools import freeze, unfreeze, weights_init_D\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T05:41:07.194201Z",
     "iopub.status.busy": "2024-05-26T05:41:07.193985Z",
     "iopub.status.idle": "2024-05-26T05:41:07.257944Z",
     "shell.execute_reply": "2024-05-26T05:41:07.257089Z",
     "shell.execute_reply.started": "2024-05-26T05:41:07.194185Z"
    }
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Config\n",
    "\n",
    "Dataset choosing in the first rows\n",
    "\n",
    "Continue training in the last rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T05:41:07.258990Z",
     "iopub.status.busy": "2024-05-26T05:41:07.258703Z",
     "iopub.status.idle": "2024-05-26T05:41:07.360857Z",
     "shell.execute_reply": "2024-05-26T05:41:07.359943Z",
     "shell.execute_reply.started": "2024-05-26T05:41:07.258970Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_LABELED = 10\n",
    "DATASET = 'fmnist2mnist'\n",
    "DATASET_PATH = '~/data/'\n",
    "IMG_SIZE = 32\n",
    "CHANNELS = 1\n",
    "\n",
    "# total batch size is BATCH_SIZE*SUBSET_SIZE\n",
    "BATCH_SIZE=32 # num of random classes in one batch\n",
    "SUBSET_SIZE=2 # num of samples in each class in 1 batch\n",
    "\n",
    "# GPU choosing\n",
    "DEVICE_IDS = [0]\n",
    "\n",
    "# whether continue training, -1 means re-training, N > -1 means continue at sdeN or iteration N\n",
    "# In this training strategy, the \"sde\" doesn't matter\n",
    "CONTINUE = {\"sde\": -1, \"iteration\": -1}\n",
    "\n",
    "# We use epsilon in [0, 1, 10]\n",
    "EPSILON = 0\n",
    "\n",
    "# N steps in the Euler-Maruyama(step number of SDE shift and noise)\n",
    "N_STEPS = 5\n",
    "\n",
    "# the step number adding noise in diffusion process style\n",
    "DIFFUSION_STEPS = 1000\n",
    "PIVOTAL_LIST = [0, 20, 50, 100]\n",
    "\n",
    "OUTER_ITERS = 5001  # MAX_STEPS = 100001\n",
    "INNER_ITERS = 10\n",
    "\n",
    "LOG_INTERVAL = 50\n",
    "PLOT_INTERVAL = 50\n",
    "CPKT_INTERVAL = 200\n",
    "\n",
    "\n",
    "# All hyperparameters below is set to the values used for the experiments, which discribed in the article\n",
    "BETA_NET_LR, SDE_LR = 1e-4, 1e-4\n",
    "BETA_NET_BETA, SDE_BETA = 0.9, 0.9\n",
    "SDE_GRADIENT_MAX_NORM = float(\"inf\")\n",
    "BETA_NET_GRADIENT_MAX_NORM = float(\"inf\")\n",
    "\n",
    "UNET_BASE_FACTOR = 128\n",
    "TIME_DIM = 128\n",
    "USE_POSITIONAL_ENCODING = True\n",
    "INTEGRAL_SCALE = 1 / (3 * IMG_SIZE * IMG_SIZE)\n",
    "\n",
    "PREDICT_SHIFT = True\n",
    "USE_GRADIENT_CHECKPOINT = False\n",
    "N_LAST_STEPS_WITHOUT_NOISE = 1\n",
    "EPSILON_SCHEDULER_LAST_ITER = 20000\n",
    "IMAGE_INPUT = True\n",
    "\n",
    "FID_EPOCHS = 1\n",
    "\n",
    "SEED = 0xBADBEEF\n",
    "assert torch.cuda.is_available()\n",
    "torch.cuda.set_device(f\"cuda:{DEVICE_IDS[0]}\")\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_subset = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7 ,8, 9])\n",
    "new_labels_source = {0:0, 1:1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:7, 8:8, 9:9}\n",
    "target_subset = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "new_labels_target = {0:0, 1:1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:7, 8:8, 9:9}\n",
    "scheduler_milestones=[1000, 2000, 3000, 4000, 5000]\n",
    "\n",
    "source_transform = Compose([\n",
    "    Resize((IMG_SIZE, IMG_SIZE)), \n",
    "    ToTensor(),\n",
    "    Normalize((0.5), (0.5)),\n",
    "])\n",
    "target_transform = source_transform\n",
    "\n",
    "if DATASET == 'mnist2kmnist':\n",
    "    source = datasets.MNIST\n",
    "    target = datasets.KMNIST\n",
    "    \n",
    "elif DATASET == 'fmnist2mnist':\n",
    "    source = datasets.FashionMNIST\n",
    "    target = datasets.MNIST\n",
    "    \n",
    "elif DATASET == 'mnist2usps':\n",
    "    source = datasets.MNIST\n",
    "    target = datasets.USPS\n",
    "    \n",
    "elif DATASET == 'mnist2mnistm':\n",
    "    source = datasets.MNIST\n",
    "    target = MNISTM\n",
    "    CHANNELS = 3\n",
    "    source_transform = Compose([\n",
    "        Resize((IMG_SIZE, IMG_SIZE)), \n",
    "        ToTensor(),\n",
    "        Normalize((0.5), (0.5)), \n",
    "        Lambda(lambda x: -x.repeat(3,1,1))])\n",
    "    target_transform = Compose([\n",
    "        Resize(IMG_SIZE),\n",
    "        ToTensor(),\n",
    "        Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T05:41:07.363030Z",
     "iopub.status.busy": "2024-05-26T05:41:07.362612Z",
     "iopub.status.idle": "2024-05-26T05:41:07.369688Z",
     "shell.execute_reply": "2024-05-26T05:41:07.368985Z",
     "shell.execute_reply.started": "2024-05-26T05:41:07.362997Z"
    }
   },
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "EXP_NAME = f\"Fix_{DATASET}_pivotal_{'_'.join(map(str, PIVOTAL_LIST))}_{current_time}\"\n",
    "\n",
    "OUTPUT_PATH = f\"../logs/{EXP_NAME}/\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    filename=os.path.join(OUTPUT_PATH, \"training.log\"),\n",
    "    format=\"%(asctime)s - [line:%(lineno)d] - %(levelname)s: %(message)s\",\n",
    "    filemode=\"w\",  # 这里的'w'代表写模式，如果用'a'，则为追加模式\n",
    ")\n",
    "\n",
    "\n",
    "logging.info(f\"{EXP_NAME = }\")\n",
    "logging.info(f\"{OUTPUT_PATH = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T05:41:07.370535Z",
     "iopub.status.busy": "2024-05-26T05:41:07.370351Z",
     "iopub.status.idle": "2024-05-26T05:41:07.376109Z",
     "shell.execute_reply": "2024-05-26T05:41:07.375540Z",
     "shell.execute_reply.started": "2024-05-26T05:41:07.370518Z"
    }
   },
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    NUM_LABELED=NUM_LABELED,\n",
    "    DATASET=DATASET,\n",
    "    DATASET_PATH=DATASET_PATH,\n",
    "    CHANNELS=CHANNELS,\n",
    "    SEED=SEED,\n",
    "    EPSILON=EPSILON,\n",
    "    N_STEPS=N_STEPS,\n",
    "    BATCH_SIZE=BATCH_SIZE,\n",
    "    SUBSET_SIZE=SUBSET_SIZE,\n",
    "    DIFFUSION_STEPS=DIFFUSION_STEPS,\n",
    "    PIVOTAL_LIST=PIVOTAL_LIST,\n",
    "    MAX_STEPS=OUTER_ITERS,\n",
    "    INNER_ITERS=INNER_ITERS,\n",
    "    LOG_INTERVAL=LOG_INTERVAL,\n",
    "    PLOT_INTERVAL=PLOT_INTERVAL,\n",
    "    CPKT_INTERVAL=CPKT_INTERVAL,\n",
    "    BETA_NET_LR=BETA_NET_LR,\n",
    "    SDE_LR=SDE_LR,\n",
    "    BETA_NET_BETA=BETA_NET_BETA,\n",
    "    SDE_BETA=SDE_BETA,\n",
    "    SDE_GRADIENT_MAX_NORM=SDE_GRADIENT_MAX_NORM,\n",
    "    BETA_NET_GRADIENT_MAX_NORM=BETA_NET_GRADIENT_MAX_NORM,\n",
    "    IMG_SIZE=IMG_SIZE,\n",
    "    UNET_BASE_FACTOR=UNET_BASE_FACTOR,\n",
    "    TIME_DIM=TIME_DIM,\n",
    "    USE_POSITIONAL_ENCODING=USE_POSITIONAL_ENCODING,\n",
    "    INTEGRAL_SCALE=INTEGRAL_SCALE,\n",
    "    PREDICT_SHIFT=PREDICT_SHIFT,\n",
    "    USE_GRADIENT_CHECKPOINT=USE_GRADIENT_CHECKPOINT,\n",
    "    N_LAST_STEPS_WITHOUT_NOISE=N_LAST_STEPS_WITHOUT_NOISE,\n",
    "    EPSILON_SCHEDULER_LAST_ITER=EPSILON_SCHEDULER_LAST_ITER,\n",
    "    IMAGE_INPUT=IMAGE_INPUT,\n",
    "    FID_EPOCHS=FID_EPOCHS,\n",
    ")\n",
    "\n",
    "with open(os.path.join(OUTPUT_PATH, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f, ensure_ascii=False)\n",
    "logging.info(f\"Config: \\n{json.dumps(config, indent=4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Function definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data and pivotal sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-05-26T05:41:07.938668Z",
     "iopub.status.busy": "2024-05-26T05:41:07.937780Z",
     "iopub.status.idle": "2024-05-26T05:41:09.632508Z",
     "shell.execute_reply": "2024-05-26T05:41:09.631963Z",
     "shell.execute_reply.started": "2024-05-26T05:41:07.938608Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_all_pivotal(\n",
    "    XY_sampler: PairedSubsetSampler,\n",
    "    batch_size: int = 4,\n",
    "    diffusion_steps: int = DIFFUSION_STEPS,\n",
    "    pivotal_list: list[int] = PIVOTAL_LIST,\n",
    ") -> list[torch.Tensor]:\n",
    "    scheduler = DDIMScheduler(num_train_timesteps=diffusion_steps)\n",
    "    pivotal_path = []\n",
    "\n",
    "    source, target = XY_sampler.sample(batch_size)\n",
    "    source, target = source.flatten(0, 1), target.flatten(0, 1)\n",
    "\n",
    "    source_list = [source]\n",
    "    target_list = [target]\n",
    "    for i in range(min(diffusion_steps, pivotal_list[-1])):\n",
    "        source = scheduler.add_noise(\n",
    "            source, torch.randn_like(source), torch.Tensor([i]).long()\n",
    "        )\n",
    "        target = scheduler.add_noise(\n",
    "            target, torch.randn_like(target), torch.Tensor([i]).long()\n",
    "        )\n",
    "        if (i + 1) in pivotal_list:\n",
    "            source_list.append(source)\n",
    "            target_list.append(target)\n",
    "\n",
    "    target_list.reverse()\n",
    "\n",
    "    pivotal_path.extend(source_list)\n",
    "    pivotal_path.extend(target_list[1:])\n",
    "\n",
    "    return pivotal_path\n",
    "\n",
    "\n",
    "def sample_step_t_pivotal(\n",
    "    XY_sampler: PairedSubsetSampler,\n",
    "    batch_size: int = 4,\n",
    "    diffusion_steps: int = DIFFUSION_STEPS,\n",
    "    pivotal_list: list[int] = PIVOTAL_LIST,\n",
    "    pivotal_step: int = 0,\n",
    "):\n",
    "    pivotal_path = sample_all_pivotal(\n",
    "        XY_sampler, batch_size, \n",
    "        diffusion_steps, pivotal_list\n",
    "    )\n",
    "    pivotal_t, pivotal_t_next = (\n",
    "        pivotal_path[pivotal_step],\n",
    "        pivotal_path[pivotal_step + 1],\n",
    "    )\n",
    "    return pivotal_t, pivotal_t_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapping plotters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-05-26T05:41:09.633980Z",
     "iopub.status.busy": "2024-05-26T05:41:09.633678Z",
     "iopub.status.idle": "2024-05-26T05:41:09.650342Z",
     "shell.execute_reply": "2024-05-26T05:41:09.649513Z",
     "shell.execute_reply.started": "2024-05-26T05:41:09.633960Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 采样关键点\n",
    "def draw_all_pivotal(\n",
    "    source: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    diffusion_steps: int = 1000,\n",
    "    pivotal_list: list[int] = [0, 20, 50, 100],\n",
    ") -> list:\n",
    "    scheduler = DDIMScheduler(num_train_timesteps=diffusion_steps)\n",
    "    pivotal_path = []\n",
    "\n",
    "    source_list = [source]\n",
    "    target_list = [target]\n",
    "    for i in range(min(diffusion_steps, pivotal_list[-1])):\n",
    "        source = scheduler.add_noise(\n",
    "            source, torch.randn_like(source), torch.Tensor([i]).long()\n",
    "        )\n",
    "        target = scheduler.add_noise(\n",
    "            target, torch.randn_like(target), torch.Tensor([i]).long()\n",
    "        )\n",
    "        if (i + 1) in pivotal_list:\n",
    "            source_list.append(source)\n",
    "            target_list.append(target)\n",
    "\n",
    "    target_list.reverse()\n",
    "\n",
    "    pivotal_path.extend(source_list)\n",
    "    pivotal_path.extend(target_list[1:])\n",
    "\n",
    "    imgs: np.ndarray = (\n",
    "        torch.stack(pivotal_path)\n",
    "        .to(\"cpu\")\n",
    "        .permute(0, 2, 3, 1)\n",
    "        .mul(0.5)\n",
    "        .add(0.5)\n",
    "        .numpy()\n",
    "        .clip(0, 1)\n",
    "    )\n",
    "    nrows, ncols = 1, len(pivotal_path)\n",
    "    fig = plt.figure(figsize=(11 * ncols, 10 * nrows))\n",
    "    for i, img in enumerate(imgs):\n",
    "        ax = fig.add_subplot(nrows, ncols, i + 1)\n",
    "        ax.imshow(img)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_title(f\"X({i})\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def draw_sub_mapping(\n",
    "    XY_sampler: PairedSubsetSampler,\n",
    "    sde: SDE,\n",
    "    plot_n_samples: int = 4,\n",
    "    diffusion_steps: int = 1000,\n",
    "    pivotal_list: list[int] = [0, 200, 500, 1000],\n",
    "    pivotal_step: int = 0,\n",
    "    saving_path: str | None = None,\n",
    "):\n",
    "    clear_output()\n",
    "    \n",
    "    subsetsize = XY_sampler.subsetsize\n",
    "    XY_sampler.subsetsize = 1\n",
    "    source, target = sample_step_t_pivotal(\n",
    "        XY_sampler,\n",
    "        plot_n_samples,\n",
    "        diffusion_steps,\n",
    "        pivotal_list,\n",
    "        pivotal_step,\n",
    "    )\n",
    "    XY_sampler.subsetsize = subsetsize\n",
    "\n",
    "    tr, _, _ = sde(source)\n",
    "    # print(f\"[Debug] {tr.shape = }\")\n",
    "    mapped = tr[:, -1, :]\n",
    "    # print(f\"[Debug] {mapped.shape = }\") # shape (plot_n_sample, 3, 32, 32)\n",
    "\n",
    "    n_imgs: np.ndarray = (\n",
    "        torch.stack([source, target, mapped])\n",
    "        .to(\"cpu\")\n",
    "        .permute(1, 0, 3, 4, 2)\n",
    "        .mul(0.5)\n",
    "        .add(0.5)\n",
    "        .numpy()\n",
    "        .clip(0, 1)\n",
    "    )  # shpae = (plot_n_samples, 3, [h, w, c]) 3->(source, target, mapped)\n",
    "    nrows, ncols = plot_n_samples, 3\n",
    "    fig = plt.figure(figsize=(10 * ncols, 10 * nrows))\n",
    "    for i, imgs in enumerate(n_imgs):\n",
    "        for j, img in enumerate(imgs):\n",
    "            ax = fig.add_subplot(plot_n_samples, 3, i * ncols + j + 1)\n",
    "            ax.imshow(img)\n",
    "            ax.set_axis_off()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if saving_path:\n",
    "        plt.savefig(os.path(saving_path))\n",
    "    plt.show()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def draw_linked_mapping(\n",
    "    XY_sampler: PairedSubsetSampler,\n",
    "    SDEs: list[SDE],\n",
    "    plot_n_samples: int = 4,\n",
    "):\n",
    "    device = next(SDEs[0].parameters()).device\n",
    "    source_dataset, target_dataset, mapped_dataset = [], [], []\n",
    "    pivotals_list = []\n",
    "    for i in range(plot_n_samples):\n",
    "        subsetsize = XY_sampler.subsetsize\n",
    "        XY_sampler.subsetsize = 1\n",
    "        source, target = XY_sampler.sample(1)\n",
    "        source = source.flatten(0,1).to(device)\n",
    "        target = target.flatten(0,1).to(device)\n",
    "        XY_sampler.subsetsize = subsetsize\n",
    "\n",
    "\n",
    "        pivotals = [source.clone().detach()]\n",
    "        for t in range(len(SDEs)):\n",
    "            x0 = pivotals[t]\n",
    "            trajectory, times, _ = SDEs[t](x0)\n",
    "            xN = trajectory[:, -1, :]\n",
    "            pivotals.append(xN)\n",
    "\n",
    "        mapped_dataset.append(pivotals[-1])\n",
    "        source_dataset.append(source)\n",
    "        target_dataset.append(target)\n",
    "\n",
    "        pivotals.append(target.clone().detach())\n",
    "        pivotals_list.append(pivotals)\n",
    "\n",
    "        fig, axes = plt.subplots(1, len(SDEs) + 2, figsize=(5 * (len(SDEs) + 2), 5))\n",
    "        axes = axes.flatten()\n",
    "        for pivotal, ax in zip(pivotals, axes):\n",
    "            img = torch.squeeze(pivotal, 0).to(\"cpu\")\n",
    "            img = img.permute(1, 2, 0).mul(0.5).add(0.5).numpy().clip(0, 1)\n",
    "            ax.imshow(img)\n",
    "\n",
    "        fig.tight_layout(pad=0.001)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return pivotals_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initalize data sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T05:41:10.037382Z",
     "iopub.status.busy": "2024-05-26T05:41:10.037115Z",
     "iopub.status.idle": "2024-05-26T05:41:15.660857Z",
     "shell.execute_reply": "2024-05-26T05:41:15.659445Z",
     "shell.execute_reply.started": "2024-05-26T05:41:10.037366Z"
    }
   },
   "outputs": [],
   "source": [
    "source_train = source(root=DATASET_PATH, train=True, download=True, transform=source_transform)\n",
    "subset_samples, labels, source_class_indicies = get_indicies_subset(source_train, \n",
    "                                                                    new_labels = new_labels_source, \n",
    "                                                                    classes=len(source_subset), \n",
    "                                                                    subset_classes=source_subset)\n",
    "source_train =  torch.utils.data.TensorDataset(torch.stack(subset_samples), \n",
    "                                               torch.LongTensor(labels))\n",
    "\n",
    "\n",
    "target_train = target(root=DATASET_PATH, train=True, download=True, transform=target_transform)   \n",
    "target_subset_samples, target_labels, target_class_indicies = get_indicies_subset(target_train, \n",
    "                                                                                  new_labels = new_labels_target, \n",
    "                                                                                  classes=len(target_subset), \n",
    "                                                                                  subset_classes=target_subset)\n",
    "target_train = torch.utils.data.TensorDataset(torch.stack(target_subset_samples), \n",
    "                                              torch.LongTensor(target_labels))\n",
    "\n",
    "train_set = SubsetGuidedDataset(source_train, target_train, \n",
    "                                num_labeled=NUM_LABELED, \n",
    "                                in_indicies = source_class_indicies, \n",
    "                                out_indicies = target_class_indicies)\n",
    "\n",
    "full_set = SubsetGuidedDataset(source_train, target_train, \n",
    "                               num_labeled='all', \n",
    "                               in_indicies = source_class_indicies, \n",
    "                               out_indicies = target_class_indicies)\n",
    "\n",
    "XY_sampler = PairedSubsetSampler(train_set, subsetsize=SUBSET_SIZE)    \n",
    "D_XY_sampler = PairedSubsetSampler(full_set, subsetsize=1)\n",
    "\n",
    "logging.info(\"Initialize data sampler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### separate fixed X, Y, show sampler data and pivotal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-05-26T05:41:15.664972Z",
     "iopub.status.busy": "2024-05-26T05:41:15.664078Z",
     "iopub.status.idle": "2024-05-26T05:41:16.402489Z",
     "shell.execute_reply": "2024-05-26T05:41:16.401691Z",
     "shell.execute_reply.started": "2024-05-26T05:41:15.664903Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_fixed, Y_fixed = D_XY_sampler.sample(10)\n",
    "X_fixed, Y_fixed = X_fixed.flatten(0, 1), Y_fixed.flatten(0, 1)\n",
    "\n",
    "draw_all_pivotal(X_fixed[0], Y_fixed[0], DIFFUSION_STEPS, PIVOTAL_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T05:41:16.403581Z",
     "iopub.status.busy": "2024-05-26T05:41:16.403379Z",
     "iopub.status.idle": "2024-05-26T05:41:18.441660Z",
     "shell.execute_reply": "2024-05-26T05:41:18.440638Z",
     "shell.execute_reply.started": "2024-05-26T05:41:16.403562Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "SDEs, BETA_NETs = [], []\n",
    "SDE_OPTs, BETA_NET_OPTs = [], []\n",
    "SDE_SCHEDULERs, BETA_NET_SCHEDULERs = [], []\n",
    "\n",
    "for i in range(len(PIVOTAL_LIST) * 2 - 2):\n",
    "    sde = CUNet(\n",
    "        CHANNELS, CHANNELS, TIME_DIM, base_factor=UNET_BASE_FACTOR\n",
    "    ).cuda()\n",
    "\n",
    "    sde = SDE(\n",
    "        shift_model=sde,\n",
    "        epsilon=EPSILON,\n",
    "        n_steps=N_STEPS,\n",
    "        time_dim=TIME_DIM,\n",
    "        n_last_steps_without_noise=N_LAST_STEPS_WITHOUT_NOISE,\n",
    "        use_positional_encoding=USE_POSITIONAL_ENCODING,\n",
    "        use_gradient_checkpoint=USE_GRADIENT_CHECKPOINT,\n",
    "        predict_shift=PREDICT_SHIFT,\n",
    "        image_input=IMAGE_INPUT,\n",
    "    ).cuda()\n",
    "    SDEs.append(sde)\n",
    "\n",
    "    beta_net = ResNet_D(IMG_SIZE, nc=CHANNELS).cuda()\n",
    "    beta_net.apply(weights_init_D)\n",
    "    BETA_NETs.append(beta_net)\n",
    "\n",
    "    sde_opt = torch.optim.Adam(\n",
    "        sde.parameters(), lr=SDE_LR, weight_decay=1e-10, betas=(SDE_BETA, 0.999)\n",
    "    )\n",
    "    beta_net_opt = torch.optim.Adam(\n",
    "        beta_net.parameters(),\n",
    "        lr=BETA_NET_LR,\n",
    "        weight_decay=1e-10,\n",
    "        betas=(BETA_NET_BETA, 0.999),\n",
    "    )\n",
    "    SDE_OPTs.append(sde_opt)\n",
    "    BETA_NET_OPTs.append(beta_net_opt)\n",
    "\n",
    "    sde_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        sde_opt, milestones=scheduler_milestones, gamma=0.5\n",
    "    )\n",
    "    beta_net_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        beta_net_opt, milestones=scheduler_milestones, gamma=0.5\n",
    "    )\n",
    "    SDE_SCHEDULERs.append(sde_scheduler)\n",
    "    BETA_NET_SCHEDULERs.append(beta_net_scheduler)\n",
    "\n",
    "\n",
    "if len(DEVICE_IDS) > 1 and CONTINUE[0] == -1 and CONTINUE[1] == -1:\n",
    "    for i in range(len(SDEs)):\n",
    "        SDEs[i] = nn.DataParallel(SDEs[i], device_ids=DEVICE_IDS)\n",
    "        BETA_NETs[i] = nn.DataParallel(BETA_NETs[i], device_ids=DEVICE_IDS)\n",
    "\n",
    "        print(\"T params:\", np.sum([np.prod(p.shape) for p in SDEs[0].parameters()]))\n",
    "        print(\n",
    "            \"D params:\", np.sum([np.prod(p.shape) for p in BETA_NETs[0].parameters()])\n",
    "        )\n",
    "\n",
    "logging.info(\"Initialize models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load weights for continue training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T05:41:18.443459Z",
     "iopub.status.busy": "2024-05-26T05:41:18.443296Z",
     "iopub.status.idle": "2024-05-26T05:41:18.450141Z",
     "shell.execute_reply": "2024-05-26T05:41:18.449403Z",
     "shell.execute_reply.started": "2024-05-26T05:41:18.443444Z"
    }
   },
   "outputs": [],
   "source": [
    "CKPT_DIR = os.path.join(OUTPUT_PATH, f\"iter{CONTINUE[\"iteration\"]}/\")\n",
    "if CONTINUE[\"iteration\"] > -1:\n",
    "    for i in range(len(SDEs)):\n",
    "        SDEs[i].load_state_dict(torch.load(os.path.join(CKPT_DIR, f\"sde{i}.pt\")))\n",
    "        SDE_OPTs[i].load_state_dict(\n",
    "            torch.load(os.path.join(CKPT_DIR, f\"sde_opt{i}.pt\"))\n",
    "        )\n",
    "        SDE_SCHEDULERs[i].load_state_dict(\n",
    "            torch.load(os.path.join(CKPT_DIR, f\"sde_scheduler{i}.pt\"))\n",
    "        )\n",
    "\n",
    "        BETA_NETs[i].load_state_dict(\n",
    "            torch.load(os.path.join(CKPT_DIR, f\"beta_net{i}.pt\"))\n",
    "        )\n",
    "        BETA_NET_OPTs[i].load_state_dict(\n",
    "            torch.load(os.path.join(CKPT_DIR, f\"beta_net_opt{i}.pt\"))\n",
    "        )\n",
    "        BETA_NET_SCHEDULERs[i].load_state_dict(\n",
    "            torch.load(os.path.join(CKPT_DIR, f\"beta_net_scheduler{i}.pt\"))\n",
    "        )\n",
    "\n",
    "        if len(DEVICE_IDS) > 1:\n",
    "            SDEs[i] = nn.DataParallel(sde, device_ids=DEVICE_IDS)\n",
    "            BETA_NETs[i] = nn.DataParallel(beta_net, device_ids=DEVICE_IDS)\n",
    "\n",
    "logging.info(f\"Continue training at iteration {CONTINUE[\"iteration\"]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def epsilon_scheduler(step):\n",
    "    return min(EPSILON, EPSILON * (step / EPSILON_SCHEDULER_LAST_ITER))\n",
    "\n",
    "\n",
    "# linked mapping training\n",
    "def training_linked_mapping(\n",
    "    XY_sampler: PairedSubsetSampler,\n",
    "    SDEs: list[SDE | torch.nn.DataParallel | torch.nn.Module],\n",
    "    BETA_NETs: list[ResNet_D | torch.nn.Module | torch.nn.Module],\n",
    "    SDE_OPTs: list[torch.optim.Optimizer],\n",
    "    BETA_NET_OPTs: list[torch.optim.Optimizer],\n",
    "    SDE_SCHEDULERs: list[torch.optim.lr_scheduler.LRScheduler],\n",
    "    BETA_NET_SCHEDULERs: list[torch.optim.lr_scheduler.LRScheduler],\n",
    "    outer_iterations: int = OUTER_ITERS,\n",
    "    inner_iterations: int = INNER_ITERS,\n",
    "    diffusion_steps: int = DIFFUSION_STEPS,\n",
    "    pivotal_list: list[int] = PIVOTAL_LIST,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    continue_sde: int = CONTINUE[\"sde\"],\n",
    "    continue_iter: int = CONTINUE[\"iteration\"]\n",
    "):\n",
    "    T = len(SDEs)\n",
    "    for t in range(T):\n",
    "        freeze(SDEs[t])\n",
    "        freeze(BETA_NETs[t])\n",
    "    for i in trange(max(continue_iter, 0), outer_iterations):\n",
    "        # interval events\n",
    "        if i % LOG_INTERVAL == 0:\n",
    "            logging.info(f\"========== Iteration={i+1}/{outer_iterations} ==========\")\n",
    "\n",
    "        if i != 0 and i % CPKT_INTERVAL == 0:\n",
    "            CKPT_DIR = os.path.join(OUTPUT_PATH, f\"iter{i}/\")\n",
    "            os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "            for tt in range(T):\n",
    "                torch.save(\n",
    "                    SDEs[tt].state_dict(),\n",
    "                    os.path.join(CKPT_DIR, f\"sde{tt}.pt\"),\n",
    "                )\n",
    "                torch.save(\n",
    "                    SDE_OPTs[tt].state_dict(), os.path.join(CKPT_DIR, f\"sde_opt{tt}.pt\")\n",
    "                )\n",
    "                torch.save(\n",
    "                    SDE_SCHEDULERs[tt].state_dict(),\n",
    "                    os.path.join(CKPT_DIR, f\"sde_scheduler{tt}.pt\"),\n",
    "                )\n",
    "\n",
    "                torch.save(\n",
    "                    BETA_NETs[tt].state_dict(),\n",
    "                    os.path.join(CKPT_DIR, f\"beta_net{tt}.pt\"),\n",
    "                )\n",
    "                torch.save(\n",
    "                    BETA_NET_OPTs[tt].state_dict(),\n",
    "                    os.path.join(CKPT_DIR, f\"beta_net_opt{tt}.pt\"),\n",
    "                )\n",
    "                torch.save(\n",
    "                    BETA_NET_SCHEDULERs[tt].state_dict(),\n",
    "                    os.path.join(CKPT_DIR, f\"beta_net_scheduler{tt}.pt\"),\n",
    "                )\n",
    "                logging.info(f\"Checkpoint saving at iteration {i}\")\n",
    "\n",
    "        if i % PLOT_INTERVAL == 0:\n",
    "            clear_output()\n",
    "            draw_linked_mapping(\n",
    "                XY_sampler,\n",
    "                SDEs,\n",
    "                NUM_LABELED,\n",
    "            )\n",
    "            if CHANNELS == 1:\n",
    "                _gray = True\n",
    "            else:\n",
    "                _gray = False\n",
    "            print('X_fixed results:')\n",
    "            fig, axes = plot_fixed_linked_sdes_images(X_fixed, Y_fixed, SDEs, 2, _gray)\n",
    "            plt.show()\n",
    "            plt.close(fig) \n",
    "            print('X_random results:')\n",
    "            fig, axes = plot_random_linked_sdes_images(XY_sampler, SDEs, 2, _gray)\n",
    "            plt.show()\n",
    "            plt.close(fig)\n",
    "        \n",
    "        # sample fixed training data for all sdes\n",
    "        mapping_list_list: list[list[torch.Tensor]] = []\n",
    "        for _ in range(inner_iterations):\n",
    "            mapping_list = sample_all_pivotal(\n",
    "                XY_sampler,\n",
    "                batch_size,\n",
    "                diffusion_steps,\n",
    "                pivotal_list,\n",
    "            )\n",
    "            mapping_list_list.append(mapping_list)\n",
    "\n",
    "        # training sde\n",
    "        for t in range(T):\n",
    "            logging.info(f\"---------- training sde{t}({t + 1}/{T}) ----------\")\n",
    "            sde, beta_net, sde_opt, beta_net_opt, sde_scheduler, beta_net_scheduler = (\n",
    "                SDEs[t],\n",
    "                BETA_NETs[t],\n",
    "                SDE_OPTs[t],\n",
    "                BETA_NET_OPTs[t],\n",
    "                SDE_SCHEDULERs[t],\n",
    "                BETA_NET_SCHEDULERs[t],\n",
    "            )\n",
    "\n",
    "            # Sub mapping training\n",
    "            # Optim beta network\n",
    "            freeze(sde)\n",
    "            unfreeze(beta_net)\n",
    "            beta_net.train()\n",
    "\n",
    "            # clear grad\n",
    "            beta_net_opt.zero_grad()\n",
    "\n",
    "            x, y = mapping_list_list[0][t], mapping_list_list[0][t + 1]\n",
    "            x, y = x.clone().detach(), y.clone().detach()\n",
    "\n",
    "            # forward\n",
    "            trajectory, times, shifts = sde(x)\n",
    "            xT = trajectory[:, -1, :]\n",
    "            # loss\n",
    "            norm = torch.norm(shifts.flatten(start_dim=2), p=2, dim=-1) ** 2\n",
    "            integral = INTEGRAL_SCALE * integrate(norm, times[0])\n",
    "            loss_beta = (-integral - beta_net(y) + beta_net(xT)).mean()\n",
    "            # backward\n",
    "            loss_beta.backward()\n",
    "            # clip gradient\n",
    "            beta_net_grad_norm = torch.nn.utils.clip_grad_norm_(  # noqa: F841\n",
    "                beta_net.parameters(), max_norm=BETA_NET_GRADIENT_MAX_NORM\n",
    "            )\n",
    "            # update weights\n",
    "            beta_net_opt.step()\n",
    "            # update beta network lr scheduler\n",
    "            beta_net_scheduler.step()\n",
    "\n",
    "            # Optim sde network\n",
    "            freeze(beta_net)\n",
    "            unfreeze(sde)\n",
    "            sde.train()\n",
    "\n",
    "            new_epsilon = epsilon_scheduler(i)\n",
    "            if len(DEVICE_IDS) > 1:\n",
    "                sde.module.set_epsilon(new_epsilon)\n",
    "            else:\n",
    "                sde.set_epsilon(new_epsilon)\n",
    "\n",
    "            # now it's same sample\n",
    "            for ii in range(inner_iterations):\n",
    "                # clear grad\n",
    "                sde_opt.zero_grad()\n",
    "\n",
    "                x, y = mapping_list_list[ii][t], mapping_list_list[ii][t + 1]\n",
    "                x, y = x.clone().detach(), y.clone().detach()\n",
    "\n",
    "                # forward\n",
    "                trajectory, times, shifts = sde(x)\n",
    "                xT = trajectory[:, -1, :]\n",
    "                # loss\n",
    "                norm = torch.norm(shifts.flatten(start_dim=2), p=2, dim=-1) ** 2\n",
    "                integral = INTEGRAL_SCALE * integrate(norm, times[0])\n",
    "                loss_sde = (integral + beta_net(y) - beta_net(xT)).mean()\n",
    "                # backward and update weights\n",
    "                loss_sde.backward()\n",
    "                # clip gradient\n",
    "                sde_gradient_norm = torch.nn.utils.clip_grad_norm_(  # noqa: F841\n",
    "                    sde.parameters(), max_norm=SDE_GRADIENT_MAX_NORM\n",
    "                )\n",
    "                # update weights\n",
    "                sde_opt.step()\n",
    "            # update sde network lr scheduler\n",
    "            sde_scheduler.step()\n",
    "        # # clear memory cache\n",
    "        # del loss_beta, loss_sde, x, y, xT\n",
    "        # gc.collect()\n",
    "        # torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T05:41:18.451316Z",
     "iopub.status.busy": "2024-05-26T05:41:18.451065Z"
    }
   },
   "outputs": [],
   "source": [
    "training_linked_mapping(\n",
    "    XY_sampler,\n",
    "    SDEs,\n",
    "    BETA_NETs,\n",
    "    SDE_OPTs,\n",
    "    BETA_NET_OPTs,\n",
    "    SDE_SCHEDULERs,\n",
    "    BETA_NET_SCHEDULERs,\n",
    "    OUTER_ITERS,\n",
    "    INNER_ITERS,\n",
    "    DIFFUSION_STEPS,\n",
    "    PIVOTAL_LIST,\n",
    "    BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving model\n",
    "\n",
    "each sub mapping model has been saved at training time, this is option code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, sde in enumerate(SDEs):\n",
    "    path = os.path.join(OUTPUT_PATH, f\"sde{i}.pt\")\n",
    "    torch.save(sde.state_dict(), path)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
