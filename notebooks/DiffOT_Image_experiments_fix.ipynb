{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Code for experiments with colored MNIST and Celeba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# This needed to use dataloaders for some datasets\n",
    "from PIL import PngImagePlugin\n",
    "from tqdm import trange\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "LARGE_ENOUGH_NUMBER = 100\n",
    "PngImagePlugin.MAX_TEXT_CHUNK = LARGE_ENOUGH_NUMBER * (1024**2)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Config\n",
    "\n",
    "Dataset choosing in the first rows\n",
    "\n",
    "Continue training in the last rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Celeba exps\n",
    "# DATASET1, DATASET1_PATH = \"CelebA_low\", \"/home/zyz/data/img_align_celeba\"\n",
    "# DATASET2, DATASET2_PATH = \"CelebA_high\", \"/home/zyz/data/img_align_celeba\"\n",
    "\n",
    "# For Colored MNIST exps\n",
    "DATASET1, DATASET1_PATH = \"MNIST-colored_2\", \"~/data/MNIST\"\n",
    "DATASET2, DATASET2_PATH = \"MNIST-colored_3\", \"~/data/MNIST\"\n",
    "\n",
    "SEED = 0xBADBEEF\n",
    "\n",
    "\n",
    "# whether continue training first for choosing pivotal t network, last for choosing training iteration\n",
    "CONTINUE = [-1, -1]\n",
    "\n",
    "# We use epsilon in [0, 1, 10]\n",
    "EPSILON = 1\n",
    "\n",
    "# N steps in the Euler-Maruyama(step number of SDE shift and noise)\n",
    "N_STEPS = 10\n",
    "BATCH_SIZE = 32\n",
    "COST = \"schrodinger\"\n",
    "\n",
    "MAX_STEPS = 2001  # MAX_STEPS = 100001\n",
    "INNER_ITERS = 10\n",
    "\n",
    "# GPU choosing\n",
    "DEVICE_IDS = [0]\n",
    "\n",
    "# the step number adding noise in diffusion process style\n",
    "HALF_STEPS = 1000\n",
    "PIVOTAL_LIST = [0, 20, 50, 100]\n",
    "\n",
    "LOG_INTERVAL = 50\n",
    "PLOT_INTERVAL = 50\n",
    "CPKT_INTERVAL = 100\n",
    "\n",
    "# All hyperparameters below is set to the values used for the experiments, which discribed in the article\n",
    "BETA_NET_LR, SDE_LR = 1e-4, 1e-4\n",
    "BETA_BETA_NET, BETA_SDE = 0.9, 0.9\n",
    "SDE_GRADIENT_MAX_NORM = float(\"inf\")\n",
    "BETA_NET_GRADIENT_MAX_NORM = float(\"inf\")\n",
    "\n",
    "\n",
    "IMG_SIZE = 32\n",
    "UNET_BASE_FACTOR = 128\n",
    "\n",
    "TIME_DIM = 128\n",
    "CONSTANT_TIME = False\n",
    "USE_POSITIONAL_ENCODING = True\n",
    "RESNET_GENERATOR = False\n",
    "INTEGRAL_SCALE = 1 / (3 * IMG_SIZE * IMG_SIZE)\n",
    "ONE_STEP_INIT_ITERS = 0\n",
    "\n",
    "PREDICT_SHIFT = True\n",
    "SMART_INTERVALS = False\n",
    "INTERVAL_SHRINK_START_TIME = 0.98\n",
    "LAST_STEP_NOISE_STD = 1e-3\n",
    "USE_GRADIENT_CHECKPOINT = False\n",
    "PREDICT_NOISE_AT_LAST_STEP = False\n",
    "N_LAST_STEPS_WITHOUT_NOISE = 1\n",
    "TRACK_VAR_INTERVAL = 10\n",
    "IMPROVED_DIFFUSION = False\n",
    "USE_CHECKPOINTS_INSIDE_MODEL = False\n",
    "EPSILON_SCHEDULER_LAST_ITER = 20000\n",
    "USE_EXPONENTIAL_AVERAGE_MODEL = False\n",
    "DISTINCT_SHIFT_MODELS = False\n",
    "IMAGE_INPUT = True\n",
    "\n",
    "DATASET1_CHANNELS = 3\n",
    "DATASET2_CHANNELS = 3\n",
    "GRAY_PLOTS = False\n",
    "STEPS_TO_SHOW = 10\n",
    "\n",
    "\n",
    "GAMMA0, GAMMA1 = 0.0, 0.333\n",
    "GAMMA_ITERS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = f\"Fix_{DATASET1}_to_{DATASET2}_pivotal_{'_'.join(map(str, PIVOTAL_LIST))}\"\n",
    "OUTPUT_PATH = f\"../logs/{EXP_NAME}/\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    filename=os.path.join(OUTPUT_PATH, \"training.log\"),\n",
    "    format=\"%(asctime)s - [line:%(lineno)d] - %(levelname)s: %(message)s\",\n",
    "    filemode=\"w\",  # 这里的'w'代表写模式，如果用'a'，则为追加模式\n",
    ")\n",
    "\n",
    "\n",
    "logging.info(f\"{EXP_NAME = }\")\n",
    "logging.info(f\"{OUTPUT_PATH = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    SEED=SEED,\n",
    "    DATASET1=DATASET1,\n",
    "    DATASET2=DATASET2,\n",
    "    INNER_ITERS=INNER_ITERS,\n",
    "    HALF_STEPS=HALF_STEPS,\n",
    "    BETA_NET_LR=BETA_NET_LR,\n",
    "    SDE_LR=SDE_LR,\n",
    "    PIVOTAL_LIST=PIVOTAL_LIST,\n",
    "    BATCH_SIZE=BATCH_SIZE,\n",
    "    UNET_BASE_FACTOR=UNET_BASE_FACTOR,\n",
    "    N_STEPS=N_STEPS,\n",
    "    EPSILON=EPSILON,\n",
    "    CONSTANT_TIME=CONSTANT_TIME,\n",
    "    USE_POSITIONAL_ENCODING=USE_POSITIONAL_ENCODING,\n",
    "    TIME_DIM=TIME_DIM,\n",
    "    RESNET_GENERATOR=RESNET_GENERATOR,\n",
    "    INTEGRAL_SCALE=INTEGRAL_SCALE,\n",
    "    ONE_STEP_INIT_ITERS=ONE_STEP_INIT_ITERS,\n",
    "    SDE_GRADIENT_MAX_NORM=SDE_GRADIENT_MAX_NORM,\n",
    "    BETA_NET_GRADIENT_MAX_NORM=BETA_NET_GRADIENT_MAX_NORM,\n",
    "    PREDICT_SHIFT=PREDICT_SHIFT,\n",
    "    SMART_INTERVALS=SMART_INTERVALS,\n",
    "    INTERVAL_SHRINK_START_TIME=INTERVAL_SHRINK_START_TIME,\n",
    "    LAST_STEP_NOISE_STD=LAST_STEP_NOISE_STD,\n",
    "    USE_GRADIENT_CHECKPOINT=USE_GRADIENT_CHECKPOINT,\n",
    "    PREDICT_NOISE_AT_LAST_STEP=PREDICT_NOISE_AT_LAST_STEP,\n",
    "    N_LAST_STEPS_WITHOUT_NOISE=N_LAST_STEPS_WITHOUT_NOISE,\n",
    "    LOG_INTERVALS=LOG_INTERVAL,\n",
    "    TRACK_VAR_INTERVAL=TRACK_VAR_INTERVAL,\n",
    "    IMPROVED_DIFFUSION=IMPROVED_DIFFUSION,\n",
    "    USE_CHECKPOINTS_INSIDE_MODEL=USE_CHECKPOINTS_INSIDE_MODEL,\n",
    "    EPSILON_SCHEDULER_LAST_ITER=EPSILON_SCHEDULER_LAST_ITER,\n",
    "    USE_EXPONENTIAL_AVERAGE_MODEL=USE_EXPONENTIAL_AVERAGE_MODEL,\n",
    "    DISTINCT_SHIFT_MODELS=DISTINCT_SHIFT_MODELS,\n",
    ")\n",
    "\n",
    "AUGMENTED_DATASETS = [\"dtd\"]\n",
    "FID_EPOCHS = 50 if DATASET1 in AUGMENTED_DATASETS else 1\n",
    "print(f\"{torch.__version__}\")\n",
    "assert torch.cuda.is_available()\n",
    "torch.cuda.set_device(f\"cuda:{DEVICE_IDS[0]}\")\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "logging.info(f\"Config: \\n{json.dumps(config, indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Function definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### network generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cunet import CUNet\n",
    "from src.enot import SDE, integrate\n",
    "from src.resnet2 import ResNet_D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data and pivotal sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from diffusers import DDIMScheduler\n",
    "\n",
    "from src import distributions\n",
    "\n",
    "\n",
    "def sample_all_pivotal(\n",
    "    source_sampler: distributions.Sampler,\n",
    "    target_sampler: distributions.Sampler,\n",
    "    batch_size: int = 4,\n",
    "    half_steps: int = 1000,\n",
    "    pivotal_list: list[int] = [0, 20, 50, 100],\n",
    ") -> list[torch.Tensor]:\n",
    "    scheduler = DDIMScheduler(num_train_timesteps=half_steps)\n",
    "    pivotal_path = []\n",
    "\n",
    "    source: torch.Tensor = source_sampler.sample(batch_size)\n",
    "    target: torch.Tensor = target_sampler.sample(batch_size)\n",
    "    source_list = [source]\n",
    "    target_list = [target]\n",
    "    for i in range(min(half_steps, pivotal_list[-1])):\n",
    "        source = scheduler.add_noise(\n",
    "            source, torch.randn_like(source), torch.Tensor([i]).long()\n",
    "        )\n",
    "        target = scheduler.add_noise(\n",
    "            target, torch.randn_like(target), torch.Tensor([i]).long()\n",
    "        )\n",
    "        if (i + 1) in pivotal_list:\n",
    "            source_list.append(source)\n",
    "            target_list.append(target)\n",
    "\n",
    "    target_list.reverse()\n",
    "\n",
    "    pivotal_path.extend(source_list)\n",
    "    pivotal_path.extend(target_list[1:])\n",
    "\n",
    "    return pivotal_path\n",
    "\n",
    "\n",
    "def sample_step_t_pivotal(\n",
    "    source_sampler: distributions.Sampler,\n",
    "    target_sampler: distributions.Sampler,\n",
    "    batch_size: int = 4,\n",
    "    half_steps: int = 1000,\n",
    "    pivotal_list: list[int] = [0, 200, 500, 1000],\n",
    "    pivotal_step: int = 0,\n",
    "):\n",
    "    pivotal_path = sample_all_pivotal(\n",
    "        source_sampler, target_sampler, batch_size, half_steps, pivotal_list\n",
    "    )\n",
    "    pivotal_t, pivotal_tadd1 = (\n",
    "        pivotal_path[pivotal_step],\n",
    "        pivotal_path[pivotal_step + 1],\n",
    "    )\n",
    "    return pivotal_t, pivotal_tadd1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapping plotters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 采样关键点\n",
    "def draw_all_pivotal(\n",
    "    source: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    half_steps: int = 1000,\n",
    "    pivotal_list: list[int] = [0, 20, 50, 100],\n",
    ") -> list:\n",
    "    scheduler = DDIMScheduler(num_train_timesteps=half_steps)\n",
    "    pivotal_path = []\n",
    "\n",
    "    source_list = [source]\n",
    "    target_list = [target]\n",
    "    for i in range(min(half_steps, pivotal_list[-1])):\n",
    "        source = scheduler.add_noise(\n",
    "            source, torch.randn_like(source), torch.Tensor([i]).long()\n",
    "        )\n",
    "        target = scheduler.add_noise(\n",
    "            target, torch.randn_like(target), torch.Tensor([i]).long()\n",
    "        )\n",
    "        if (i + 1) in pivotal_list:\n",
    "            source_list.append(source)\n",
    "            target_list.append(target)\n",
    "\n",
    "    target_list.reverse()\n",
    "\n",
    "    pivotal_path.extend(source_list)\n",
    "    pivotal_path.extend(target_list[1:])\n",
    "\n",
    "    imgs: np.ndarray = (\n",
    "        torch.stack(pivotal_path)\n",
    "        .to(\"cpu\")\n",
    "        .permute(0, 2, 3, 1)\n",
    "        .mul(0.5)\n",
    "        .add(0.5)\n",
    "        .numpy()\n",
    "        .clip(0, 1)\n",
    "    )\n",
    "    nrows, ncols = 1, len(pivotal_path)\n",
    "    fig = plt.figure(figsize=(11 * ncols, 10 * nrows))\n",
    "    for i, img in enumerate(imgs):\n",
    "        ax = fig.add_subplot(nrows, ncols, i + 1)\n",
    "        ax.imshow(img)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_title(f\"X({i})\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def draw_sub_mapping(\n",
    "    source_sampler: distributions.Sampler,\n",
    "    target_sampler: distributions.Sampler,\n",
    "    sde: SDE,\n",
    "    plot_n_samples: int = 4,\n",
    "    half_steps: int = 1000,\n",
    "    pivotal_list: list[int] = [0, 200, 500, 1000],\n",
    "    pivotal_step: int = 0,\n",
    "    saving_path: str | None = None,\n",
    "):\n",
    "    clear_output()\n",
    "    source, target = sample_step_t_pivotal(\n",
    "        source_sampler,\n",
    "        target_sampler,\n",
    "        plot_n_samples,\n",
    "        half_steps,\n",
    "        pivotal_list,\n",
    "        pivotal_step,\n",
    "    )\n",
    "\n",
    "    tr, _, _ = sde(source)\n",
    "    # print(f\"[Debug] {tr.shape = }\")\n",
    "    mapped = tr[:, -1, :]\n",
    "    # print(f\"[Debug] {mapped.shape = }\") # shape (plot_n_sample, 3, 32, 32)\n",
    "\n",
    "    n_imgs: np.ndarray = (\n",
    "        torch.stack([source, target, mapped])\n",
    "        .to(\"cpu\")\n",
    "        .permute(1, 0, 3, 4, 2)\n",
    "        .mul(0.5)\n",
    "        .add(0.5)\n",
    "        .numpy()\n",
    "        .clip(0, 1)\n",
    "    )  # shpae = (plot_n_samples, 3, [h, w, c]) 3->(source, target, mapped)\n",
    "    nrows, ncols = plot_n_samples, 3\n",
    "    fig = plt.figure(figsize=(10 * ncols, 10 * nrows))\n",
    "    for i, imgs in enumerate(n_imgs):\n",
    "        for j, img in enumerate(imgs):\n",
    "            ax = fig.add_subplot(plot_n_samples, 3, i * ncols + j + 1)\n",
    "            ax.imshow(img)\n",
    "            ax.set_axis_off()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if saving_path:\n",
    "        plt.savefig(os.path(saving_path))\n",
    "    plt.show()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def draw_linked_mapping(\n",
    "    source_sampler: distributions.Sampler,\n",
    "    target_sampler: distributions.Sampler,\n",
    "    SDEs: list[SDE],\n",
    "    plot_n_samples: int = 4,\n",
    "):\n",
    "    device = next(SDEs[0].parameters()).device\n",
    "    source_dataset, target_dataset, mapped_dataset = [], [], []\n",
    "    pivotals_list = []\n",
    "    for i in range(plot_n_samples):\n",
    "        source, target = (\n",
    "            source_sampler.sample(1).to(device),\n",
    "            target_sampler.sample(1).to(device),\n",
    "        )\n",
    "\n",
    "        pivotals = [source.clone().detach()]\n",
    "        for t in range(len(SDEs)):\n",
    "            x0 = pivotals[t]\n",
    "            trajectory, times, _ = SDEs[t](x0)\n",
    "            xN = trajectory[:, -1, :]\n",
    "            pivotals.append(xN)\n",
    "\n",
    "        mapped_dataset.append(pivotals[-1])\n",
    "        source_dataset.append(source)\n",
    "        target_dataset.append(target)\n",
    "\n",
    "        pivotals.append(target.clone().detach())\n",
    "        pivotals_list.append(pivotals)\n",
    "\n",
    "        fig, axes = plt.subplots(1, len(SDEs) + 2, figsize=(5 * (len(SDEs) + 2), 5))\n",
    "        axes = axes.flatten()\n",
    "        for pivotal, ax in zip(pivotals, axes):\n",
    "            img = torch.squeeze(pivotal).to(\"cpu\")\n",
    "            img = img.permute(1, 2, 0).mul(0.5).add(0.5).numpy().clip(0, 1)\n",
    "            ax.imshow(img)\n",
    "\n",
    "        fig.tight_layout(pad=0.001)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return pivotals_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import freeze, unfreeze, weights_init_D\n",
    "\n",
    "\n",
    "def epsilon_scheduler(step):\n",
    "    return min(EPSILON, EPSILON * (step / EPSILON_SCHEDULER_LAST_ITER))\n",
    "\n",
    "\n",
    "# submapping training\n",
    "def training_sub_mapping(\n",
    "    source_sampler: distributions.Sampler,\n",
    "    target_sampler: distributions.Sampler,\n",
    "    sde: SDE | torch.nn.DataParallel | torch.nn.Module,\n",
    "    beta_net: ResNet_D | torch.nn.Module | torch.nn.Module,\n",
    "    sde_opt: torch.optim.Optimizer,\n",
    "    beta_net_opt: torch.optim.Optimizer,\n",
    "    sde_scheduler: torch.optim.lr_scheduler,\n",
    "    beta_net_scheduler: torch.optim.lr_scheduler,\n",
    "    inner_iterations: int = 10,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    epsilon: int = EPSILON,\n",
    "):\n",
    "    # Optim beta network\n",
    "    freeze(sde)\n",
    "    unfreeze(beta_net)\n",
    "    beta_net.train()\n",
    "\n",
    "    # clear grad\n",
    "    beta_net_opt.zero_grad()\n",
    "\n",
    "    x0, xT = source_sampler.sample(batch_size), target_sampler.sample(batch_size)\n",
    "    x0.requires_grad_()\n",
    "\n",
    "    # forward\n",
    "    trajectory, times, shifts = sde(x0)\n",
    "    xT_mapped = trajectory[:, -1, :]\n",
    "    # loss\n",
    "    norm = torch.norm(shifts.flatten(start_dim=2), p=2, dim=-1) ** 2\n",
    "    integral = INTEGRAL_SCALE * integrate(norm, times[0])\n",
    "    loss_beta = (-integral - beta_net(xT) + beta_net(xT_mapped)).mean()\n",
    "    # backward\n",
    "    loss_beta.backward()\n",
    "    # clip gradient\n",
    "    beta_net_grad_norm = torch.nn.utils.clip_grad_norm_(  # noqa: F841\n",
    "        beta_net.parameters(), max_norm=BETA_NET_GRADIENT_MAX_NORM\n",
    "    )\n",
    "    # update weights\n",
    "    beta_net_opt.step()\n",
    "    # update beta network lr scheduler\n",
    "    beta_net_scheduler.step()\n",
    "    # clear memory cache\n",
    "    del loss_beta, x0, xT, xT_mapped\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Optim sde network\n",
    "    freeze(beta_net)\n",
    "    unfreeze(sde)\n",
    "    sde.train()\n",
    "    if len(DEVICE_IDS) > 1:\n",
    "        sde.module.set_epsilon(epsilon)\n",
    "    else:\n",
    "        sde.set_epsilon(epsilon)\n",
    "\n",
    "    # now it's same sample\n",
    "    for i in range(inner_iterations):\n",
    "        # clear grad\n",
    "        sde_opt.zero_grad()\n",
    "\n",
    "        x0, xT = source_sampler.sample(batch_size), target_sampler.sample(batch_size)\n",
    "        x0.requires_grad_()\n",
    "\n",
    "        # forward\n",
    "        trajectory, times, shifts = sde(x0)\n",
    "        xT_mapped = trajectory[:, -1, :]\n",
    "        # loss\n",
    "        norm = torch.norm(shifts.flatten(start_dim=2), p=2, dim=-1) ** 2\n",
    "        integral = INTEGRAL_SCALE * integrate(norm, times[0])\n",
    "        loss_sde = (integral + beta_net(xT) - beta_net(xT_mapped)).mean()\n",
    "        # backward and update weights\n",
    "        loss_sde.backward()\n",
    "        # clip gradient\n",
    "        sde_gradient_norm = torch.nn.utils.clip_grad_norm_(  # noqa: F841\n",
    "            sde.parameters(), max_norm=SDE_GRADIENT_MAX_NORM\n",
    "        )\n",
    "        # update weights\n",
    "        sde_opt.step()\n",
    "    # update sde network lr scheduler\n",
    "    sde_scheduler.step()\n",
    "    # clear memory cache\n",
    "    del loss_sde, x0, xT, xT_mapped\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# linked mapping training\n",
    "def training_linked_mapping(\n",
    "    source_sampler: distributions.Sampler,\n",
    "    target_sampler: distributions.Sampler,\n",
    "    SDEs: list[SDE | torch.nn.DataParallel | torch.nn.Module],\n",
    "    BETA_NETs: list[ResNet_D | torch.nn.Module | torch.nn.Module],\n",
    "    SDE_OPTs: list[torch.optim.Optimizer],\n",
    "    BETA_NET_OPTs: list[torch.optim.Optimizer],\n",
    "    SDE_SCHEDULERs: list[torch.optim.lr_scheduler.LRScheduler],\n",
    "    BETA_NET_SCHEDULERs: list[torch.optim.lr_scheduler.LRScheduler],\n",
    "    iterations: int = 1000,\n",
    "    inner_iterations: int = 10,\n",
    "    half_steps: int = 1000,\n",
    "    pivotal_list: list[int] = [0, 200, 500, 1000],\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "):\n",
    "    T = len(SDEs)\n",
    "    for i in trange(iterations):\n",
    "        # interval events\n",
    "        if i % LOG_INTERVAL == 0:\n",
    "            logging.info(f\"iteration={i}\")\n",
    "\n",
    "        if i % CPKT_INTERVAL == 0:\n",
    "            CKPT_DIR = os.path.join(OUTPUT_PATH, f\"iter{i}/\")\n",
    "            os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "            for t in range(T):\n",
    "                torch.save(\n",
    "                    SDEs[t].state_dict(),\n",
    "                    os.path.join(CKPT_DIR, f\"sde{t}.pt\"),\n",
    "                )\n",
    "\n",
    "        if i % PLOT_INTERVAL == 0:\n",
    "            clear_output()\n",
    "            draw_linked_mapping(\n",
    "                source_sampler,\n",
    "                target_sampler,\n",
    "                SDEs,\n",
    "                1,\n",
    "            )\n",
    "\n",
    "        # training sde\n",
    "        for t in range(T):\n",
    "            logging.info(f\"training {t}-th sde\")\n",
    "            sde, beta_net, sde_opt, beta_net_opt, sde_scheduler, beta_net_scheduler = (\n",
    "                SDEs[t],\n",
    "                BETA_NETs[t],\n",
    "                SDE_OPTs[t],\n",
    "                BETA_NET_OPTs[t],\n",
    "                SDE_SCHEDULERs[t],\n",
    "                BETA_NET_SCHEDULERs[t],\n",
    "            )\n",
    "\n",
    "            # Sub mapping training\n",
    "            # Optim beta network\n",
    "            freeze(sde)\n",
    "            unfreeze(beta_net)\n",
    "            beta_net.train()\n",
    "\n",
    "            # clear grad\n",
    "            beta_net_opt.zero_grad()\n",
    "\n",
    "            x0, xT = sample_step_t_pivotal(\n",
    "                source_sampler,\n",
    "                target_sampler,\n",
    "                batch_size,\n",
    "                half_steps,\n",
    "                pivotal_list,\n",
    "                t,\n",
    "            )\n",
    "            x0.requires_grad_()\n",
    "\n",
    "            # forward\n",
    "            trajectory, times, shifts = sde(x0)\n",
    "            xT_mapped = trajectory[:, -1, :]\n",
    "            # loss\n",
    "            norm = torch.norm(shifts.flatten(start_dim=2), p=2, dim=-1) ** 2\n",
    "            integral = INTEGRAL_SCALE * integrate(norm, times[0])\n",
    "            loss_beta = (-integral - beta_net(xT) + beta_net(xT_mapped)).mean()\n",
    "            # backward\n",
    "            loss_beta.backward()\n",
    "            # clip gradient\n",
    "            beta_net_grad_norm = torch.nn.utils.clip_grad_norm_(  # noqa: F841\n",
    "                beta_net.parameters(), max_norm=BETA_NET_GRADIENT_MAX_NORM\n",
    "            )\n",
    "            # update weights\n",
    "            beta_net_opt.step()\n",
    "            # update beta network lr scheduler\n",
    "            beta_net_scheduler.step()\n",
    "            # clear memory cache\n",
    "            del loss_beta, x0, xT, xT_mapped\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Optim sde network\n",
    "            freeze(beta_net)\n",
    "            unfreeze(sde)\n",
    "            sde.train()\n",
    "\n",
    "            new_epsilon = epsilon_scheduler(i)\n",
    "            if len(DEVICE_IDS) > 1:\n",
    "                sde.module.set_epsilon(new_epsilon)\n",
    "            else:\n",
    "                sde.set_epsilon(new_epsilon)\n",
    "\n",
    "            # now it's same sample\n",
    "            for ii in range(inner_iterations):\n",
    "                # clear grad\n",
    "                sde_opt.zero_grad()\n",
    "\n",
    "                x0, xT = sample_step_t_pivotal(\n",
    "                    source_sampler,\n",
    "                    target_sampler,\n",
    "                    batch_size,\n",
    "                    half_steps,\n",
    "                    pivotal_list,\n",
    "                    t,\n",
    "                )\n",
    "                x0.requires_grad_()\n",
    "\n",
    "                # forward\n",
    "                trajectory, times, shifts = sde(x0)\n",
    "                xT_mapped = trajectory[:, -1, :]\n",
    "                # loss\n",
    "                norm = torch.norm(shifts.flatten(start_dim=2), p=2, dim=-1) ** 2\n",
    "                integral = INTEGRAL_SCALE * integrate(norm, times[0])\n",
    "                loss_sde = (integral + beta_net(xT) - beta_net(xT_mapped)).mean()\n",
    "                # backward and update weights\n",
    "                loss_sde.backward()\n",
    "                # clip gradient\n",
    "                sde_gradient_norm = torch.nn.utils.clip_grad_norm_(  # noqa: F841\n",
    "                    sde.parameters(), max_norm=SDE_GRADIENT_MAX_NORM\n",
    "                )\n",
    "                # update weights\n",
    "                sde_opt.step()\n",
    "            # update sde network lr scheduler\n",
    "            sde_scheduler.step()\n",
    "            # clear memory cache\n",
    "            del loss_sde, x0, xT, xT_mapped\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initalize data sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import load_dataset\n",
    "\n",
    "X_sampler, X_test_sampler = load_dataset(\n",
    "    DATASET1, DATASET1_PATH, img_size=IMG_SIZE, batch_size=BATCH_SIZE, num_workers=8\n",
    ")\n",
    "Y_sampler, Y_test_sampler = load_dataset(\n",
    "    DATASET2, DATASET2_PATH, img_size=IMG_SIZE, batch_size=BATCH_SIZE, num_workers=8\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "clear_output()\n",
    "\n",
    "logging.info(\"Data sampler OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### separate fixed X, Y, show sampler data and pivotal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_fixed, Y_fixed = X_sampler.sample(10), Y_sampler.sample(10)\n",
    "\n",
    "X_test_fixed, Y_test_fixed = X_test_sampler.sample(10), Y_test_sampler.sample(10)\n",
    "\n",
    "draw_all_pivotal(X_fixed[0], Y_fixed[0], HALF_STEPS, PIVOTAL_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDEs, BETA_NETs = [], []\n",
    "SDE_OPTs, BETA_NET_OPTs = [], []\n",
    "SDE_SCHEDULERs, BETA_NET_SCHEDULERs = [], []\n",
    "\n",
    "for i in range(len(PIVOTAL_LIST) * 2 - 2):\n",
    "    sde = CUNet(\n",
    "        DATASET1_CHANNELS, DATASET2_CHANNELS, TIME_DIM, base_factor=UNET_BASE_FACTOR\n",
    "    ).cuda()\n",
    "\n",
    "    sde = SDE(\n",
    "        shift_model=sde,\n",
    "        epsilon=EPSILON,\n",
    "        n_steps=N_STEPS,\n",
    "        time_dim=TIME_DIM,\n",
    "        n_last_steps_without_noise=N_LAST_STEPS_WITHOUT_NOISE,\n",
    "        use_positional_encoding=USE_POSITIONAL_ENCODING,\n",
    "        use_gradient_checkpoint=USE_GRADIENT_CHECKPOINT,\n",
    "        predict_shift=PREDICT_SHIFT,\n",
    "        image_input=IMAGE_INPUT,\n",
    "    ).cuda()\n",
    "    SDEs.append(sde)\n",
    "\n",
    "    beta_net = ResNet_D(IMG_SIZE, nc=DATASET2_CHANNELS).cuda()\n",
    "    beta_net.apply(weights_init_D)\n",
    "    BETA_NETs.append(beta_net)\n",
    "\n",
    "    sde_opt = torch.optim.Adam(\n",
    "        sde.parameters(), lr=SDE_LR, weight_decay=1e-10, betas=(BETA_SDE, 0.999)\n",
    "    )\n",
    "    beta_net_opt = torch.optim.Adam(\n",
    "        beta_net.parameters(),\n",
    "        lr=BETA_NET_LR,\n",
    "        weight_decay=1e-10,\n",
    "        betas=(BETA_BETA_NET, 0.999),\n",
    "    )\n",
    "    SDE_OPTs.append(sde_opt)\n",
    "    BETA_NET_OPTs.append(beta_net_opt)\n",
    "\n",
    "    sde_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        sde_opt, milestones=[15000, 25000, 40000, 55000, 70000], gamma=0.5\n",
    "    )\n",
    "    beta_net_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        beta_net_opt, milestones=[15000, 25000, 40000, 55000, 70000], gamma=0.5\n",
    "    )\n",
    "    SDE_SCHEDULERs.append(sde_scheduler)\n",
    "    BETA_NET_SCHEDULERs.append(beta_net_scheduler)\n",
    "\n",
    "\n",
    "if len(DEVICE_IDS) > 1 and CONTINUE[0] == -1 and CONTINUE[1] == -1:\n",
    "    for i in range(len(SDEs)):\n",
    "        SDEs[i] = nn.DataParallel(SDEs[i], device_ids=DEVICE_IDS)\n",
    "        BETA_NETs[i] = nn.DataParallel(BETA_NETs[i], device_ids=DEVICE_IDS)\n",
    "\n",
    "        print(\"T params:\", np.sum([np.prod(p.shape) for p in SDEs[0].parameters()]))\n",
    "        print(\n",
    "            \"D params:\", np.sum([np.prod(p.shape) for p in BETA_NETs[0].parameters()])\n",
    "        )\n",
    "\n",
    "logging.info(\"Network init\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: setting continue training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONTINUE[0] > -1 or CONTINUE[1] > -1:\n",
    "    for i in range(CONTINUE[0] + 1):\n",
    "        sde, sde_opt, sde_scheduler = SDEs[i], SDE_OPTs[i], SDE_SCHEDULERs[i]\n",
    "        beta_net, beta_net_opt, beta_net_scheduler = (\n",
    "            BETA_NETs[i],\n",
    "            BETA_NET_OPTs[i],\n",
    "            BETA_NET_SCHEDULERs[i],\n",
    "        )\n",
    "\n",
    "        sde_opt.load_state_dict(\n",
    "            torch.load(\n",
    "                os.path.join(OUTPUT_PATH, f\"T_opt_{SEED}_sub{i}_{CONTINUE[1]}.pt\")\n",
    "            )\n",
    "        )\n",
    "        sde_scheduler.load_state_dict(\n",
    "            torch.load(\n",
    "                os.path.join(OUTPUT_PATH, f\"T_scheduler_{SEED}_sub{i}_{CONTINUE[1]}.pt\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        sde.load_state_dict(\n",
    "            torch.load(os.path.join(OUTPUT_PATH, f\"T_{SEED}_sub{i}_{CONTINUE[1]}.pt\"))\n",
    "        )\n",
    "        beta_net.load_state_dict(\n",
    "            torch.load(os.path.join(OUTPUT_PATH, f\"D_{SEED}_sub{i}_{CONTINUE[1]}.pt\"))\n",
    "        )\n",
    "\n",
    "        if len(DEVICE_IDS) > 1:\n",
    "            SDEs[i] = nn.DataParallel(sde, device_ids=DEVICE_IDS)\n",
    "            BETA_NETs[i] = nn.DataParallel(beta_net, device_ids=DEVICE_IDS)\n",
    "\n",
    "        beta_net_opt.load_state_dict(\n",
    "            torch.load(\n",
    "                os.path.join(OUTPUT_PATH, f\"D_opt_{SEED}_sub{i}_{CONTINUE[1]}.pt\")\n",
    "            )\n",
    "        )\n",
    "        beta_net_scheduler.load_state_dict(\n",
    "            torch.load(\n",
    "                os.path.join(OUTPUT_PATH, f\"D_scheduler_{SEED}_sub{i}_{CONTINUE[1]}.pt\")\n",
    "            )\n",
    "        )\n",
    "    logging.info(\n",
    "        f\"Continue training at {CONTINUE[0]}-th sde iter{CONTINUE[1]} status load OK\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_linked_mapping(\n",
    "    X_sampler,\n",
    "    Y_sampler,\n",
    "    SDEs,\n",
    "    BETA_NETs,\n",
    "    SDE_OPTs,\n",
    "    BETA_NET_OPTs,\n",
    "    SDE_SCHEDULERs,\n",
    "    BETA_NET_SCHEDULERs,\n",
    "    MAX_STEPS,\n",
    "    INNER_ITERS,\n",
    "    HALF_STEPS,\n",
    "    PIVOTAL_LIST,\n",
    "    BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving model\n",
    "\n",
    "each sub mapping model has been saved at training time, this is option code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, sde in enumerate(SDEs):\n",
    "    path = os.path.join(OUTPUT_PATH, f\"sde{i}.pt\")\n",
    "    torch.save(sde.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metric\n",
    "\n",
    "TODO: splite to other notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initalize data sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import load_dataset\n",
    "\n",
    "X_sampler, X_test_sampler = load_dataset(\n",
    "    DATASET1, DATASET1_PATH, img_size=IMG_SIZE, batch_size=BATCH_SIZE, num_workers=1\n",
    ")\n",
    "Y_sampler, Y_test_sampler = load_dataset(\n",
    "    DATASET2, DATASET2_PATH, img_size=IMG_SIZE, batch_size=BATCH_SIZE, num_workers=1\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "clear_output()\n",
    "\n",
    "logging.info(\"Data sampler OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDEs, BETA_NETs = [], []\n",
    "SDE_OPTs, BETA_NET_OPTs = [], []\n",
    "SDE_SCHEDULERs, BETA_NET_SCHEDULERs = [], []\n",
    "\n",
    "for i in range(len(PIVOTAL_LIST) * 2 - 2):\n",
    "    sde = CUNet(\n",
    "        DATASET1_CHANNELS, DATASET2_CHANNELS, TIME_DIM, base_factor=UNET_BASE_FACTOR\n",
    "    ).cuda()\n",
    "\n",
    "    sde = SDE(\n",
    "        shift_model=sde,\n",
    "        epsilon=EPSILON,\n",
    "        n_steps=N_STEPS,\n",
    "        time_dim=TIME_DIM,\n",
    "        n_last_steps_without_noise=N_LAST_STEPS_WITHOUT_NOISE,\n",
    "        use_positional_encoding=USE_POSITIONAL_ENCODING,\n",
    "        use_gradient_checkpoint=USE_GRADIENT_CHECKPOINT,\n",
    "        predict_shift=PREDICT_SHIFT,\n",
    "        image_input=IMAGE_INPUT,\n",
    "    ).cuda()\n",
    "    SDEs.append(sde)\n",
    "\n",
    "    beta_net = ResNet_D(IMG_SIZE, nc=DATASET2_CHANNELS).cuda()\n",
    "    beta_net.apply(weights_init_D)\n",
    "    BETA_NETs.append(beta_net)\n",
    "\n",
    "    sde_opt = torch.optim.Adam(\n",
    "        sde.parameters(), lr=SDE_LR, weight_decay=1e-10, betas=(BETA_SDE, 0.999)\n",
    "    )\n",
    "    beta_net_opt = torch.optim.Adam(\n",
    "        beta_net.parameters(),\n",
    "        lr=BETA_NET_LR,\n",
    "        weight_decay=1e-10,\n",
    "        betas=(BETA_BETA_NET, 0.999),\n",
    "    )\n",
    "    SDE_OPTs.append(sde_opt)\n",
    "    BETA_NET_OPTs.append(beta_net_opt)\n",
    "\n",
    "    sde_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        sde_opt, milestones=[15000, 25000, 40000, 55000, 70000], gamma=0.5\n",
    "    )\n",
    "    beta_net_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        beta_net_opt, milestones=[15000, 25000, 40000, 55000, 70000], gamma=0.5\n",
    "    )\n",
    "    SDE_SCHEDULERs.append(sde_scheduler)\n",
    "    BETA_NET_SCHEDULERs.append(beta_net_scheduler)\n",
    "\n",
    "\n",
    "if len(DEVICE_IDS) > 1:\n",
    "    for i in range(len(SDEs)):\n",
    "        SDEs[i] = nn.DataParallel(SDEs[i], device_ids=DEVICE_IDS)\n",
    "        BETA_NETs[i] = nn.DataParallel(BETA_NETs[i], device_ids=DEVICE_IDS)\n",
    "\n",
    "        print(\"T params:\", np.sum([np.prod(p.shape) for p in SDEs[0].parameters()]))\n",
    "        print(\n",
    "            \"D params:\", np.sum([np.prod(p.shape) for p in BETA_NETs[0].parameters()])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_DIR = os.path.join(OUTPUT_PATH, \"iter2000/\")\n",
    "for i, sde in enumerate(SDEs):\n",
    "    path = os.path.join(CKPT_DIR, f\"sde{i}.pt\")\n",
    "    print(f\"{path = }\")\n",
    "    sde.load_state_dict(torch.load(path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trans example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pivotals_list shape (plot_n_samples, n_steps, (RGB image))\n",
    "pivotals_list = draw_linked_mapping(\n",
    "    X_sampler,\n",
    "    Y_sampler,\n",
    "    SDEs,\n",
    "    plot_n_samples=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pivotals in pivotals_list:\n",
    "    fig, axes = plt.subplots(1, 8, figsize=(40, 5))\n",
    "    axes = axes.flatten()\n",
    "    for pivotal, ax in zip(pivotals, axes):\n",
    "        img = torch.squeeze(pivotal).to(\"cpu\")\n",
    "        img = img.permute(1, 2, 0).mul(0.5).add(0.5).numpy().clip(0, 1)\n",
    "        ax.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load FID stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../stats/{}_{}_test.json\".format(DATASET2, IMG_SIZE)\n",
    "with open(filename, \"r\") as fp:\n",
    "    data_stats = json.load(fp)\n",
    "    mu_data, sigma_data = data_stats[\"mu\"], data_stats[\"sigma\"]\n",
    "del data_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate FID\n",
    "\n",
    "TODO:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate ACC\n",
    "\n",
    "TODO:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
